{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0730ea-1316-4f07-8457-825283980e95",
   "metadata": {},
   "source": [
    "# Project One Code \n",
    "\n",
    "## Econ 1680: MLTA and Econ\n",
    "\n",
    "#### Name: Nadya Tan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af5d5c61-dff6-4e56-8811-10646aef170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba512fce",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01d69b1-a1bc-4ba4-bdd6-a24759329f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microdata = pd.read_csv('micro_world_139countries.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "88f74001-1ef2-4466-a3f0-b412e905d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns to keep \n",
    "selected = ['economycode', 'wgt','female', 'age', 'educ', 'inc_q', 'emp_in', 'urbanicity_f2f', 'account', 'account_fin', 'account_mob', 'fin2', 'fin4', 'fin5', 'fin6', 'fin7', 'fin8', 'fin9', 'fin10', 'fin13a', 'saved', 'borrowed', 'receive_wages', 'receive_transfers', 'receive_pension', 'receive_agriculture', 'pay_utilities', 'remittances', 'mobileowner', 'internetaccess', 'anydigpayment', 'merchantpay_dig' ]\n",
    "# new df \n",
    "df_selected = df_microdata[selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7967c0a8-d8d2-4426-b181-9b7a1b955d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143887, 32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ded9a-1c3b-4349-83f6-5ae5f7eb827e",
   "metadata": {},
   "source": [
    "## One-hot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eec12546-ddc3-426e-967a-3881d27cb816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these variables need maps\n",
    "# educ - change to 3 columns with 0-1 - if value is 4-5, put na in all 3 cols \n",
    "educ_dummies = pd.get_dummies(df_selected['educ'], prefix='educ').astype(int)\n",
    "educ_dummies = educ_dummies.drop(columns=['educ_4', 'educ_5'])\n",
    "# Fill NaN values for rows where educ was 4 or 5\n",
    "educ_dummies.loc[df_selected['educ'].isin([4, 5]), :] = np.nan\n",
    "\n",
    "# Convert inc_q to 5 columns\n",
    "inc_q_dummies = pd.get_dummies(df_selected['inc_q'], prefix='inc_q').astype(int)\n",
    "df_selected = pd.concat([df_selected, educ_dummies, inc_q_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d23b68cd-997e-4a99-9cc6-032b8320393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert female to 0-1\n",
    "df_selected['female'] = df_selected['female'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Convert emp_in to 0-1\n",
    "df_selected['emp_in'] = df_selected['emp_in'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Convert urbanicity_f2f to 0-1\n",
    "df_selected['urbanicity_f2f'] = df_selected['urbanicity_f2f'].apply(lambda x: 1 if x == 2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f3904fa-ee33-497b-a246-0cf198dfebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert these other financial variables to 0-1 and na for 3-4 \n",
    "other_cols = [col for col in df_selected.columns if col in ['fin2', 'fin4', 'fin5', 'fin6', 'fin7', 'fin8', 'fin9', 'fin10', 'fin13a','mobileowner', 'internetaccess']]\n",
    "for col in other_cols:\n",
    "    df_selected[col] = df_selected[col].apply(lambda x: 1 if x == 1 else (0 if x == 2 else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ddec655-7142-4ab7-becb-0a547ae9f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_value(value):\n",
    "    # did not perform the act\n",
    "    if value == 4:\n",
    "        return 0\n",
    "    # performed using cash\n",
    "    elif value == 2:\n",
    "        return 1\n",
    "    # performed using account \n",
    "    elif value == 1:\n",
    "        return 2\n",
    "    else:\n",
    "        return np.nan \n",
    "\n",
    "# Create new columns with scaled values\n",
    "df_selected['receive_wages_scaled'] = df_selected['receive_wages'].apply(scale_value)\n",
    "df_selected['receive_transfers_scaled'] = df_selected['receive_transfers'].apply(scale_value)\n",
    "df_selected['receive_pension_scaled'] = df_selected['receive_pension'].apply(scale_value)\n",
    "df_selected['receive_agriculture_scaled'] = df_selected['receive_agriculture'].apply(scale_value)\n",
    "df_selected['pay_utilities_scaled'] = df_selected['pay_utilities'].apply(scale_value)\n",
    "\n",
    "# Replace old column with 0-1 \n",
    "def replace_value(value):\n",
    "    if value in [1, 2, 3]:\n",
    "        return 1\n",
    "    elif value == 4:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan  # NaN for values 5\n",
    "columns_to_process = ['receive_wages', 'receive_transfers', 'receive_pension', 'receive_agriculture', 'pay_utilities']\n",
    "for column in columns_to_process:\n",
    "    df_selected[column] = df_selected[column].apply(replace_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05c729e5-27fd-45d9-b38e-46d0dddefee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_value_remittances(value):\n",
    "    # did not perform the act\n",
    "    if value == 5:\n",
    "        return 0\n",
    "    # performed using cash or OTC\n",
    "    elif value in [2, 3]:\n",
    "        return 1\n",
    "    # performed using account \n",
    "    elif value == 1:\n",
    "        return 2\n",
    "    else:\n",
    "        return np.nan \n",
    "df_selected['remittances_scaled'] = df_selected['remittances'].apply(scale_value)\n",
    "\n",
    "def replace_value_remittances(value):\n",
    "    if value in [1, 2, 3, 4]:\n",
    "        return 1\n",
    "    elif value == 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df_selected['remittances'] = df_selected['remittances'].apply(replace_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbcdbe6d-8937-4f2e-8c74-70c4729fffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# economycode \n",
    "economycode_dummies = pd.get_dummies(df_selected['economycode']).astype(int)\n",
    "df_selected = pd.concat([df_selected, economycode_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "02afc3e0-e82f-42ea-8a03-a5432edc0ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143887, 185)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f21ee8af-7290-4af2-bdae-48a8efa9d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all NaNs \n",
    "columns_to_process = ['economycode', 'wgt','female', 'age', 'educ', 'inc_q', 'emp_in', 'urbanicity_f2f', 'account', 'account_fin', 'account_mob', 'fin2', 'fin4', 'fin5', 'fin6', 'fin7', 'fin8', 'fin9', 'fin10', 'fin13a', 'saved', 'borrowed', 'receive_wages', 'receive_transfers', 'receive_pension', 'receive_agriculture', 'pay_utilities', 'remittances', 'mobileowner', 'internetaccess', 'anydigpayment', 'merchantpay_dig','receive_wages_scaled', 'receive_transfers_scaled','receive_pension_scaled','receive_agriculture_scaled','pay_utilities_scaled', 'remittances_scaled'  ]\n",
    "\n",
    "# Drop rows where any value in the specified columns is NaN\n",
    "df_selected = df_selected.dropna(subset=columns_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16ee1c90-da73-4ddf-bcbc-d11f01c85f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1495, 185)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5dd885db-5587-46db-8c1b-57db73983420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.to_csv('microdata_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e32308f-c078-4606-890a-bc5ad0f060f9",
   "metadata": {},
   "source": [
    "## Summary Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adef670-c421-4e68-b79a-569695139598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5553406-8879-4c21-bbce-ef2751d32113",
   "metadata": {},
   "source": [
    "# Comparing Regressions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a516512",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_fls['y']\n",
    "X=df_fls.drop(columns=['y'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1680)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f096406b",
   "metadata": {},
   "source": [
    "#### 19. Run an OLS regression on the training data. Produce the summary statistics using .summary() and paste them here:\n",
    "\n",
    "Summary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb964d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      y   R-squared (uncentered):                   0.982\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.949\n",
      "Method:                 Least Squares   F-statistic:                              30.22\n",
      "Date:                Wed, 21 Feb 2024   Prob (F-statistic):                    7.94e-13\n",
      "Time:                        19:58:44   Log-Likelihood:                         -26.936\n",
      "No. Observations:                  64   AIC:                                      135.9\n",
      "Df Residuals:                      23   BIC:                                      224.4\n",
      "Df Model:                          41                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "===============================================================================\n",
      "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "GDPsh560       -1.1411      0.273     -4.180      0.000      -1.706      -0.576\n",
      "Confuncious     7.9154      1.734      4.565      0.000       4.329      11.502\n",
      "Life Exp        0.1305      0.029      4.533      0.000       0.071       0.190\n",
      "Equip Inv      13.8234      5.328      2.594      0.016       2.801      24.846\n",
      "SubSahara      -1.6995      0.843     -2.017      0.056      -3.443       0.044\n",
      "Muslim         -0.2057      1.184     -0.174      0.864      -2.655       2.244\n",
      "Rule of Law     0.7503      0.823      0.912      0.371      -0.951       2.452\n",
      "Yrs Open       -0.5034      0.838     -0.601      0.554      -2.236       1.229\n",
      "Eco Org         0.1699      0.154      1.103      0.281      -0.149       0.488\n",
      "Protestants    -0.4537      0.848     -0.535      0.598      -2.208       1.300\n",
      "NEquip Inv      3.1816      2.451      1.298      0.207      -1.889       8.252\n",
      "Mining          3.3508      1.810      1.852      0.077      -0.393       7.094\n",
      "LatAmerica     -1.2484      0.693     -1.802      0.085      -2.681       0.185\n",
      "PrSc Enroll     0.7715      1.114      0.693      0.496      -1.533       3.076\n",
      "Buddha          0.9916      0.827      1.199      0.243      -0.720       2.703\n",
      "Bl Mkt Pm      -0.6037      0.475     -1.271      0.216      -1.586       0.379\n",
      "Catholic        0.2370      0.685      0.346      0.732      -1.179       1.653\n",
      "Civl Lib       -0.2272      0.270     -0.843      0.408      -0.785       0.331\n",
      "Hindu         -11.7084      3.496     -3.349      0.003     -18.941      -4.475\n",
      "Pr Exports      0.2834      0.903      0.314      0.757      -1.585       2.152\n",
      "Pol Rights      0.1224      0.219      0.558      0.582      -0.332       0.577\n",
      "R FEX Dist     -0.0033      0.004     -0.866      0.395      -0.011       0.005\n",
      "Age            -0.0023      0.004     -0.598      0.556      -0.010       0.006\n",
      "War Dummy      -0.2807      0.291     -0.963      0.345      -0.884       0.322\n",
      "English %      -0.9101      0.620     -1.468      0.156      -2.193       0.373\n",
      "Foreign %      -0.0854      0.461     -0.185      0.855      -1.040       0.869\n",
      "Lab Force       0.0443      0.014      3.069      0.005       0.014       0.074\n",
      "Spanish Col     0.8778      0.581      1.511      0.144      -0.324       2.080\n",
      "EthnoL Frac     2.0654      0.633      3.264      0.003       0.756       3.374\n",
      "std(BMP)       -0.0189      0.013     -1.460      0.158      -0.046       0.008\n",
      "French Col      1.0065      0.561      1.794      0.086      -0.154       2.167\n",
      "Abs Lat         0.0152      0.181      0.084      0.934      -0.359       0.390\n",
      "Work/Pop       -1.2590      1.068     -1.179      0.250      -3.468       0.950\n",
      "High Enroll   -21.7819      6.238     -3.492      0.002     -34.687      -8.877\n",
      "Pop g          -9.7668     27.058     -0.361      0.721     -65.741      46.208\n",
      "Brit Col        0.9744      0.556      1.752      0.093      -0.176       2.125\n",
      "Outwar Or      -0.5844      0.241     -2.429      0.023      -1.082      -0.087\n",
      "Jewish        -24.6295     38.060     -0.647      0.524    -103.363      54.104\n",
      "Rev & Coup      0.8127      0.589      1.379      0.181      -0.406       2.032\n",
      "%Publ Edu      22.0264     15.552      1.416      0.170     -10.146      54.199\n",
      "Area           -0.0264      0.082     -0.322      0.750      -0.196       0.143\n",
      "==============================================================================\n",
      "Omnibus:                        1.816   Durbin-Watson:                   2.285\n",
      "Prob(Omnibus):                  0.403   Jarque-Bera (JB):                1.775\n",
      "Skew:                          -0.332   Prob(JB):                        0.412\n",
      "Kurtosis:                       2.527   Cond. No.                     7.37e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[3] The condition number is large, 7.37e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "ols_result = sm.OLS(y_train, X_train).fit()\n",
    "print(ols_result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e805d",
   "metadata": {},
   "source": [
    "#### 20. Run a LASSO regression on the training data and search over different alpha parameters in using alphas=np.linspace(1e-6, 1, num=50). Hint: reference the review session example for coding LASSO.\n",
    "20a. What is the optimal alpha that you find?\n",
    "\n",
    "Answer: 0.020409\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e83a66cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02040914285714286\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "lassoReg = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
    "\n",
    "# Define parameter grid to search over using grid search\n",
    "alphas = np.linspace(1e-6, 1, num=50)\n",
    "params = {'lasso__alpha': alphas}\n",
    "\n",
    "# Set up the grid search\n",
    "gsLasso = GridSearchCV(lassoReg, params, n_jobs=-1, cv=10)\n",
    "\n",
    "# Fit gs to data\n",
    "gsLasso.fit(X, y)\n",
    "\n",
    "# Check best alpha\n",
    "print(gsLasso.best_params_['lasso__alpha'])\n",
    "best_lasso_alpha = gsLasso.best_params_['lasso__alpha']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0439645",
   "metadata": {},
   "source": [
    "20b. Insert a plot of alphas on the x axis and cv_errors on the y axis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa14505e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbL0lEQVR4nO3de7RdZXnv8e8vF4hoEJAdIEDY0BERSTXQEKgoJUoGAh6CR7SQDMTbQYbGQ2stpILaKrXR2p5C0QMppy0CAsqlyWkDcqloWiQhaZBbTEkBQySQzUWDKGCSp3/MuePKzlpzzb0uc67L7zPGGnvNtd615jMJYz97vs97UURgZmZWy5iyAzAzs87mRGFmZpmcKMzMLJMThZmZZXKiMDOzTOPKDqAd9t577xgcHCw7DDOzrrFq1apnI2Kg2ns9mSgGBwdZuXJl2WGYmXUNST+p9Z67nszMLJMThZmZZXKiMDOzTE4UZmaWyYnCzMwyOVHksGnzy3zgih+y6cWXyw7FzKxwThQ5XHrXo9z3xPNceuejZYdiZla4npxH0SqHXnQrr2zZtv34muXruWb5enYdN4a1F59UYmRmZsXxHUWGZefP4tTpk5kwPvnPNGH8GOZMn8yyC2aVHJmZWXGcKDJM2n0CE3cdxytbtrHruDG8smUbE3cdx6SJE8oOzcysMO56quPZX7zCvKMPYu7MKXxrxXqGXNA2sz6jXtwKdcaMGVHEWk+bNr/M/OtWc9ncI3yXYWZdTdKqiJhR7T13PTXBo6HMrB+466kBHg1lZv3EdxQN8GgoM+snpSYKSe+WtFbSOkkLqrw/T9ID6eMeSW8tI86RPBrKzPpJaV1PksYCXwdmAxuA+yQtiYhHKpo9DvxeRLwg6SRgEXB08dHuzKOhzKxflFmjmAmsi4jHACRdD8wBtieKiLinov29wAGFRpjhirN+Mzjg4tOmlRiJmVl7ldn1tD/wZMXxhvS1Wj4K3FrrTUnnSFopaeXQ0FCLQjQzszIThaq8VnVSh6RZJIniglpfFhGLImJGRMwYGKi6P7iZmTWgzK6nDcCBFccHAE+NbCTpLcCVwEkR8VxBsZmZWarMO4r7gKmSDpa0C3AGsKSygaQpwM3AWRHxnyXEaGbW90pLFBGxBZgPfBdYA3w7Ih6WdK6kc9NmnwfeAHxD0v2S2r8uRwt4oyMz6yWlzsyOiKXA0hGvXV7x/GPAx4qOq1mVS3tc/N7fLjscM7OmeAmPFvLSHmbWi7yERwt5aQ8z60VOFC3kpT3MrBe566nFvLSHmfUab1xkZmbeuMjMzBrnRFEwz7Ews27jRFEwb59qZt3GxeyCeI6FmXUr31EUxHMszKxbOVEUxHMszKxbueupQJ5jYWbdyPMozMzM8ygsUWtorofsmlkWJ4oek/VLv9bQXA/ZNbMs7nrqMRfd8iDXrljPvJlTtu+FMXJobj3DQ3Y3bX6Z+det5rK5R7jobtbjsrqeXMzuEVnzNJadP4uLl67h9oef5uVfb2PC+DGcePi+fPy4Q7j8B4/t9PqFpxwGeAMmM0u466lDjLZOMLJ91jyNWkNz3zz59VVff8dXvsfggn/hmuXriUiSzuCCf+HQi25tKFYz625OFB1itHWCke3rzdMYHpp7yyeOZd7RBzH0i1dqvl5vcqBrGmb9xTWKktWqH9Ra2iOr/fGHDjAwccIO8zSuOKtql2NdF97yIN9asZ5dxo7h1a3bmDdzCt9ZtWFUsZpZ98iqUThRlGzT5per1g8uPOUwJk2csFNBuV77Vvn41St3SjpfmjOtkHObWfFczO5g9bqMRhaUi1oKpPJO5OLTpm1/Xu/cHill1nucKDpAtaU9skYxHX/oQGlLgdRbhsQjpcx6j7ueOlRRXUytMtpai5l1Fi/h0YW6bbVZL6Nu1rtKTRSS3i1praR1khZUeV+SLk3ff0DSkWXEWZZaQ1o7Ub3E5rkXZt2rtBqFpLHA14HZwAbgPklLIuKRimYnAVPTx9HA/01/9oVaBeVOlVW/cO3CrHuVVqOQ9LvAn0bEienxnwBExF9UtLkCuDsirkuP1wLHR8TGrO/uhRpFr3Dtwqw7dGqNYn/gyYrjDelro20DgKRzJK2UtHJoaKilgVrjXLsw635lJgpVeW3k7U2eNsmLEYsiYkZEzBgYGGg6OGuNbivKm9nOypxHsQE4sOL4AOCpBtpYh/MWsGbdrcxEcR8wVdLBwE+BM4C5I9osAeZLup6kiP3zevUJ6zzdVpQ3sx2V1vUUEVuA+cB3gTXAtyPiYUnnSjo3bbYUeAxYB/wd8IlSgrW28bBZs85X6hIeEbGUJBlUvnZ5xfMAPll0XFYcD5s163xe68lKkbWWlYfNmnWWzK4nSWMkva2oYKx/eNisWffITBQRsQ34q4JisT7iYbNm3SNPMft2Se+TVG1Og1nDumktK7N+VncJD0kvAq8FtgK/IpkEFxGxe/vDa4yX8Oh+3gDJrFhNLeERERMjYkxEjI+I3dPjjk0S1hsqR0OZWblyjXqSdCpwXHp4d0T8c/tCsn7m0VBmnafuHYWkhcB5wCPp47z0NbOW82gos86T547iZGB6OgIKSVcBq4GdNhoya5ZHQ5l1nrxLeOxR8fz1bYjDbLus0VBe8sOseHnuKL4MrJb0PZIRT8cBf9LWqKyvZS0i6CU/zIqXmSgkjQG2AccAR5Ekigsi4ukCYjPbzkVus/LkmZk9PyI2RsSSiFjsJGFlcJHbrDx5ahR3SPqMpAMl7TX8aHtkZhXyFLldvzBrjzw1io+kPyuX+w7gkNaHY1ZbvZ3yXL8wa4/MJTzSGsX7I+KG4kJqnpfw6C8j6xfDhusXXg7ErL6Gl/BIaxTeOMg6Wr36hZcDMWtOnq6nOyR9BrgBeGn4xYh4vm1RmY1CrfrFO77yPY+UMmuBPMXsj5DcVfwAWJU+3K9jHaXaJL16dxoufpvlU/eOIiIOLiIQs2bUmqSXNVLKxW+zfGomCknnR8RX0+fvj4jvVLz35Yj4bBEBmjWj2kgpT94zG52ao54k/UdEHDnyebXjTuNRT5Zl0+aXuXjpGm5/+Gle/vU2Jowfw4mH78uFpxzGpIkTPErK+lKjo55U43m1Y7OuUW/ynkdJme0oq0YRNZ5XOzbrKu6SMssvq+tpK8lwWAGvAX45/BYwISLGN3zSZAmQG4BB4AngAxHxwog2BwLfBPYlWZhwUURckuf73fVkjXCXlPWzhrqeImJsxR7Z49Lnw8cNJ4nUAuCuiJgK3EX1TZC2AH8UEYeRrF77SUlvbvK8ZjW5S8qsuswlPNp2UmktcHxEbJS0H8k+3IfW+cxi4LKIuKPe9/uOwhr18atXMjBxwg5dUnevHcpcIgTw3YZ1vaw7ijwzs9thn4jYCJAmi0lZjSUNAkcAywuIzfpYtfkYWV1Swzwnw3pZ2xKFpDtJ6gsjXTjK73kdcBPwBxGxOaPdOcA5AFOmTBnNKcwyZXVJuQBu/SDvntmjFhEnRMS0Ko/FwDNplxPpz03VvkPSeJIkcW1E3FznfIsiYkZEzBgYGGj15Vifq7WPt5cJsX6QNTP7RTKGwUbE7k2cdwlwNrAw/bm4yvkF/D9gTUT8dRPnMmtarSVCRlMAd5eUdau6xWxJXwSeBq4mGRo7D5g4vLxHQyeV3gB8G5gCrCfZ8+J5SZOBKyPiZElvB5YBD5IMjwX4bEQsrff9LmZbkRotgJt1kqxidp5EsTwijq73WidxorCyeU6GdZuGNy5KbZU0T9JYSWMkzQO2tjZEs97iORnWS/LcUQwClwDHktQs/p1kBNIT7Q6uUb6jsE7gLinrJk11PXUjJwrrVO6Ssk7VVNeTpDdKukvSQ+nxWyRd1OogzfqBu6SsG+Xpevo+8MfAFRFxRPraQxExLfODJfIdhXUyd0lZJ2p2CY/dImJFMq1huy0ticysDzW6TIi7pawseUY9PSvpt0gn30k6HdjY1qjM+ky9Lilwt5SVJ88dxSeBRcCbJP0UeJxk0p2ZtVC1zZQArydlpcusUUgaCyyMiD+W9FpgTES8WFh0DXKNwnpJvZFSZq3Q8KiniNgK/E76/KVuSBJmvaZet5QXHrR2y1OjWC1piaSzJP3P4UfbIzOz7WqtXguuXVj75Rke+w9VXo6I+Eh7Qmqeu56sH4ysXQxz7cIa0dTw2Ij4cOtDMrNmLTt/VuaQWg+ntVapmygkTQA+ChwObP+/rZPvKMz6gffCsKLkGR57NfBj4ETgiyRDY9e0Mygzy6fakFoPp7VWy1OjWB0RR0h6ICLekm5P+t2IeGcxIY6eaxTWzzyc1hrR7H4Uv05//kzSNOD1wGCLYjOzFvNwWmu1PIlikaQ9gc+R7HX9CNDwNqhm1n4eTmut5P0ozPqEh9NalqaGx0r6fLXXI+KLzQZmZsWpN5zWrJY8XU8vVTy2AifhGoVZ13HtwhqVZ8LdX1UeS/oaSa3CzLpMrRVqwfMurLZR1yjSwvaKiJjanpCa5xqFWX6uXRg0v2f2g5IeSB8PA2uBS1odpJmVY9n5szh1+mQmjE9+HUwYP4Y50yez7IJZJUdmnSLPzOz3VDzfAjwTEd4K1axH5Nldz/pbnmL2ixWPXwG7S9pr+NHISdPP3iHp0fTnnhltx0paLemfGzmXmdWXNe8CXOjud3mW8HgCOBB4ARCwB7A+fTsi4pBRn1T6KvB8RCyUtADYMyIuqNH208AMYPeIeE+1NiO5RmHWWhfd8iDXrljPvJlTXOjuUU3NowBuA5ZExNL0y04CToiIP2oipjnA8enzq4C7gZ0ShaQDgFOAPwc+3cT5zKwBXmDQIF/X01HDSQIgIm4Ffq/J8+4TERvT79sITKrR7m+A84Gdh2SYWdu50G2QL1E8K+kiSYOSDpJ0IfBcvQ9JulPSQ1Uec/IEJuk9wKaIWJWz/TmSVkpaOTQ0lOcjZlaHJ+kZ5EsUZwIDwC3AP5H89X9mvQ9FxAkRMa3KYzHwjKT9ANKfm6p8xbHAqWmN5HrgnZKuyTjfooiYEREzBgYGclyWmeXhBQZtVBPu0tFJP4smVxKU9JfAcxXF7L0i4vyM9scDn3Ex26wzeJJe72lowp2kz0t6U/p8V0n/CqwjuRs4ocmYFgKzJT0KzE6PkTRZ0tLMT5pZ6erVLtwl1Vuyup5+n2QWNsDZadtJJIXsLzdz0oh4LiLeFRFT05/Pp68/FREnV2l/d967CTNrv9Hs123dL2t47KsVXUwnAtdFxFZgjaQ8w2rNrId5v+7+kfUL/5V069NngFnAZyre262tUZlZx7virN90Z1982jQge7/u4ffnX7eay+Ye4SVCukhW19N5wI3Aj4H/ExGPA0g6GVhdQGxm1mXcJdWbvBWqmbXUx69eycDECTt0Sd29dsijpDpc1qgnJwoza7usLil3QXWGpvajMDNrlmd4dzcnCjMrhGd4d69cXU+S3gYMUjFKKiK+2b6wmuOuJ7Pu4BnenaPZrVCvBr4GvB04Kn1U/TIzs9Hw6rTdIc/EuRnAm5td38nMbKQ827B67kX58tQoHgL2bXcgZtaf6m3D6vpF+fJshfo9YDqwAtj+LxgRp7Y1sia4RmHW/Vy/KFazW6H+aWvDMTOrb9n5szKXA7Hi1E0UEfH9IgIxM6uUp35hxcgz6ukYSfdJ+oWkVyVtlbS5iODMrL9l1S88Sa84ebqeLgPOAL5DMgLqg8DUdgZlZgbVV6gdVlnkvvi9v110aH0l174SEbFO0th0P4p/kHRPm+MyM6vKe14UL8/w2F9K2gW4X9JXJf0h8No2x2VmVpUn6RUvT6I4K203H3gJOBB4XzuDMjOrxUXu4tVNFBHxE0DAfhHxZxHx6YhY1/7QzMyqc5G7WHkm3P0PkrWedomIgyVNB77oCXdm1okuuuVBrl2xnnkzp7jIPQqtmHA3E7gbICLulzTYquDMzFrBRe72yVOj2BIRP297JGZmTXCRu31yLQooaS4wVtJUSX8LeHismXUU76LXPnkSxaeAw0kWBLwO2Az8QRtjMjNriHfRa49cO9y1/KTSXsANJLvmPQF8ICJeqNJuD+BKYBoQwEci4of1vt/FbDMb5lVo82momC1pSdaXNjnqaQFwV0QslLQgPb6gSrtLgNsi4vR00t9uTZzTzPqQV6FtXtaop98FniTpblpOMpeiVeYAx6fPryIZUbVDopC0O3Ac8CGAiHgVeLWFMZhZH8hTu/AOetmyahT7Ap8l6fa5BJgNPBsR32/B0uP7RMRGgPTnpCptDgGGSNaWWi3pSkk1lw6RdI6klZJWDg0NNRmemfUS1y6ak6tGIWlX4EzgL0km2/1tjs/cSfUtVC8EroqIPSravhARe474/AzgXuDYiFgu6RJgc0R8rt65XaMws3pcu9hRwxPu0gRxCkmSGAQuBW7Oc9KIOCHje5+RtF9EbJS0H7CpSrMNwIaIWJ4e30hSyzAza5prF/nV7HqSdBXJfIkjgT+LiKMi4ksR8dMWnHcJcHb6/Gxg8cgGEfE08KSkQ9OX3gU80oJzm5l5ccFRyKpRnAW8ETgPuEfS5vTxYgt2uFsIzJb0KEntYyGApMmSlla0+xRwraQHgOnAl5s8r5nZdlm1C/AkvWGlzKNoN9cozKwV+mmBwWYXBTQz6yteYHBHeZbwMDPrK15gcEdOFGZmI7jQvSMnCjOzKryL3m+4mG1mNkq9WOR2MdvMrAX6tcjtriczs5z6tcjtRGFmllO/7qLnRGFmNgr9uBKti9lmZk3qhZVos4rZvqMwM2tSr9cunCjMzJrU6xP0nCjMzFqglyfouUZhZtZm3TBBzxPuzMxK0CsT9Nz1ZGbWJr1S5HaiMDNrk14pcrvrycysjYaL3HNnTuFbK9Yz1IUFbRezzcxKtGnzy8y/bjWXzT2i1DsNT7gzM+tQ3bDsh7uezMxK0E0jonxHYWZWgm4aEeVEYWZWgm4aEeVEYWZWkm5Z9sOjnszMOlDRy3503BIekvYCbgAGgSeAD0TEC1Xa/SHwMSCAB4EPR0T56dXMrE06schdVtfTAuCuiJgK3JUe70DS/sD/BmZExDRgLHBGoVGamRWsE4vcZSWKOcBV6fOrgNNqtBsHvEbSOGA34Kn2h2ZmVp5OLHKXNY9in4jYCBARGyVNGtkgIn4q6WvAeuBXwO0RcXutL5R0DnAOwJQpU9oTtZlZATpt2Y+2FbMl3QnsW+WtC4GrImKPirYvRMSeIz6/J3AT8PvAz4DvADdGxDX1zu1itpn1qnYt+VHKEh4RcUJETKvyWAw8I2m/NLj9gE1VvuIE4PGIGIqIXwM3A29rV7xmZt2gjCU/yup6WgKcDSxMfy6u0mY9cIyk3Ui6nt4F+DbBzPpSmaOhyipmLwRmS3oUmJ0eI2mypKUAEbEcuBH4D5KhsWOAReWEa2ZWrjJHQ5VyRxERz5HcIYx8/Sng5IrjLwBfKDA0M7OOVOZoKK8ea2bWJcoaDeUlPMzMzBsXmZn1unYuIuhEYWbWA9o5bNY1CjOzLlbEsFnfUZiZdbEihs06UZiZdbEihs2668nMrMu1e9ish8eamZmHx5qZWeOcKCp00mbmZmadwomiQhnL95qZdToXs+nMzczNzDqF7yjozM3Mzcw6hRMFnbmZuZlZp3DXU6rTNjM3M+sUnkdhZmaeR2FmZo1zojAzs0xOFGZmlsmJwszMMjlRmJlZJicKMzPL1JPDYyUNAT9p8ON7A8+2MJxu4Gvuff12veBrHq2DImKg2hs9mSiaIWllrbHEvcrX3Pv67XrB19xK7noyM7NMThRmZpbJiWJni8oOoAS+5t7Xb9cLvuaWcY3CzMwy+Y7CzMwyOVGYmVmmvkwUkt4taa2kdZIWVHlfki5N339A0pFlxNlKOa55XnqtD0i6R9Jby4izlepdc0W7oyRtlXR6kfG1Q55rlnS8pPslPSzp+0XH2Go5/t9+vaT/L+lH6TV/uIw4W0XS30vaJOmhGu+3/vdXRPTVAxgL/BdwCLAL8CPgzSPanAzcCgg4BlhedtwFXPPbgD3T5yf1wzVXtPtXYClwetlxF/DvvAfwCDAlPZ5UdtwFXPNnga+kzweA54Fdyo69iWs+DjgSeKjG+y3//dWPdxQzgXUR8VhEvApcD8wZ0WYO8M1I3AvsIWm/ogNtobrXHBH3RMQL6eG9wAEFx9hqef6dAT4F3ARsKjK4NslzzXOBmyNiPUBEdPt157nmACZKEvA6kkSxpdgwWycifkByDbW0/PdXPyaK/YEnK443pK+Ntk03Ge31fJTkL5JuVveaJe0PvBe4vMC42inPv/MbgT0l3S1plaQPFhZde+S55suAw4CngAeB8yJiWzHhlaLlv7/6cc9sVXlt5BjhPG26Se7rkTSLJFG8va0RtV+ea/4b4IKI2Jr8sdn18lzzOOB3gHcBrwF+KOneiPjPdgfXJnmu+UTgfuCdwG8Bd0haFhGb2xxbWVr++6sfE8UG4MCK4wNI/tIYbZtukut6JL0FuBI4KSKeKyi2dslzzTOA69MksTdwsqQtEfFPhUTYenn/3342Il4CXpL0A+CtQLcmijzX/GFgYSQd+OskPQ68CVhRTIiFa/nvr37seroPmCrpYEm7AGcAS0a0WQJ8MB09cAzw84jYWHSgLVT3miVNAW4Gzurivy4r1b3miDg4IgYjYhC4EfhEFycJyPf/9mLgHZLGSdoNOBpYU3CcrZTnmteT3EEhaR/gUOCxQqMsVst/f/XdHUVEbJE0H/guyYiJv4+IhyWdm75/OckImJOBdcAvSf4i6Vo5r/nzwBuAb6R/YW+JLl55M+c195Q81xwRayTdBjwAbAOujIiqwyy7Qc5/5y8B/yjpQZJumQsiomuXH5d0HXA8sLekDcAXgPHQvt9fXsLDzMwy9WPXk5mZjYIThZmZZXKiMDOzTE4UZmaWyYnCzMwyOVGYNUHSeyWFpDelx4O1VvWs+EzdNmadxInCrDlnAv9GMtHLrCc5UZg1SNLrgGNJ1sbaKVFI+pCkxZJuS/dL+ELF22Ml/V26P8Ltkl6TfuZ/Sbov3TvhpnT2tFmpnCjMGncacFu65MnzNTaImQnMA6YD75c0PNt9KvD1iDgc+BnwvvT1myPiqIh4K8nSGh9tX/hm+ThRmDXuTJL9D0h/nlmlzR0R8VxE/IpkLa3hVXkfj4j70+ergMH0+TRJy9LlJuYBh7cjcLPR6Lu1nsxaQdIbSJatniYpSNYZCuAbI5qOXCNn+PiVite2kiz5DfCPwGkR8SNJHyJZ08esVL6jMGvM6SS7iB2UrkB7IPA4O+8MOFvSXmkN4jTg3+t870Rgo6TxJHcUZqVzojBrzJnALSNeu4lkf+ZK/wZcTbJxzk0RsbLO934OWA7cAfy4+TDNmufVY83aJO06mhER88uOxawZvqMwM7NMvqMwM7NMvqMwM7NMThRmZpbJicLMzDI5UZiZWSYnCjMzy/Tf+TQu6nKOXW0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the alpha values and corresponding mean test scores from the grid search results\n",
    "alphas = np.linspace(1e-6, 1, num=50)\n",
    "cv_errors = gsLasso.cv_results_['mean_test_score']\n",
    "\n",
    "plt.plot(alphas, cv_errors,'*')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1ef90",
   "metadata": {},
   "source": [
    "#### 21. Run a Ridge Regression on the training data and search over different alpha parameters in using alphas=np.linspace(1e-6, 1, num=50). Hint: reference the review session example for coding Ridge regressions.\n",
    "21a. What is the optimal alpha that you find? \n",
    "\n",
    "Answer here: 0.20408242857142858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cb30753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20408242857142858\n"
     ]
    }
   ],
   "source": [
    "# Construct vector of alpha values\n",
    "alphas = np.linspace(1e-6, 1, num=50)\n",
    "\n",
    "# Construct vectors to store mean prediction errors and coefficients\n",
    "cv_errs = []\n",
    "coefs = []\n",
    "MSE = 10\n",
    "best_ridge_alpha = 0\n",
    "\n",
    "# Loop for running ridge regression for different values of alpha\n",
    "for a in alphas:\n",
    "    \n",
    "    # define pipeline object\n",
    "    ridgeReg = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha= a * X_train.shape[0]))\n",
    "    # run Ridge regression\n",
    "    ridgeReg.fit(X_train, y_train)\n",
    "    # obtain predicted values of output\n",
    "    y_pred = ridgeReg.predict(X_test)\n",
    "    # compute mean squared error\n",
    "    cv_errs.append(mean_squared_error(y_test, y_pred))\n",
    "    # store coefficients\n",
    "    coefs.append(ridgeReg['ridge'].coef_)\n",
    "    \n",
    "    # store value of alpha that minimizes the mean squared error\n",
    "    if mean_squared_error(y_test, y_pred) < MSE: #np.mean((y_pred - y_test)**2)\n",
    "        MSE = mean_squared_error(y_test, y_pred)\n",
    "        best_ridge_alpha = a\n",
    "        \n",
    "print(best_ridge_alpha)\n",
    "    \n",
    "# Create dataframe for storing coefficients\n",
    "coefs = pd.DataFrame(coefs, columns=X.columns)\n",
    "coefs.set_index(alphas, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999f8a8",
   "metadata": {},
   "source": [
    "21b. Insert a plot of alphas on the x axis and cv_errros on the y axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9837dada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Mean Squared Error')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the MSE against the values of alphas\n",
    "plt.plot(alphas, cv_errs, '*')\n",
    "plt.xlabel(r'$\\alpha$')\n",
    "plt.ylabel('Mean Squared Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb94f7",
   "metadata": {},
   "source": [
    "#### 22. Compare the regression coefficients from the three different approaches using:\n",
    "``` python\n",
    "coef_comp=pd.DataFrame({'var':X.columns, 'val_ols':olsReg.params.tolist(), 'val_lasso':lassoReg.coef_, 'var_ridge':ridgeReg.coef_})\n",
    "```\n",
    "\n",
    "22a. Insert the coef_comp table here:\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92d16f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;ridge&#x27;, Ridge(alpha=1.6326594285714287))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;ridge&#x27;, Ridge(alpha=1.6326594285714287))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_mean=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=1.6326594285714287)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(with_mean=False)),\n",
       "                ('ridge', Ridge(alpha=1.6326594285714287))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = best_lasso_alpha\n",
    "lassoReg = make_pipeline(StandardScaler(with_mean=False), Lasso(alpha= alpha * np.sqrt(X_train.shape[0])))\n",
    "lassoReg.fit(X_train, y_train)\n",
    "\n",
    "alpha = best_ridge_alpha\n",
    "ridgeReg = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha= alpha * np.sqrt(X_train.shape[0])))\n",
    "ridgeReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7246c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_comp=pd.DataFrame({'var':X.columns, 'val_ols':ols_result.params.tolist(), 'val_lasso':lassoReg[-1].coef_, 'var_ridge':ridgeReg[-1].coef_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a67d6953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            var    val_ols  val_lasso  var_ridge\n",
      "0      GDPsh560  -1.141114  -0.000000  -1.030650\n",
      "1   Confuncious   7.915376   0.510285   0.524869\n",
      "2      Life Exp   0.130482   0.000000   0.722722\n",
      "3     Equip Inv  13.823435   0.651147   0.431589\n",
      "4     SubSahara  -1.699480  -0.108605  -0.613402\n",
      "5        Muslim  -0.205658   0.048257   0.050455\n",
      "6   Rule of Law   0.750318   0.000000   0.244289\n",
      "7      Yrs Open  -0.503423   0.234918  -0.018356\n",
      "8       Eco Org   0.169880   0.008327   0.175540\n",
      "9   Protestants  -0.453677  -0.137091  -0.227882\n",
      "10   NEquip Inv   3.181588   0.097685   0.155285\n",
      "11       Mining   3.350791   0.015638   0.278279\n",
      "12   LatAmerica  -1.248353  -0.000000  -0.343418\n",
      "13  PrSc Enroll   0.771504   0.000000   0.294868\n",
      "14       Buddha   0.991624   0.314402   0.170280\n",
      "15    Bl Mkt Pm  -0.603701  -0.000000  -0.213988\n",
      "16     Catholic   0.236960  -0.000000  -0.130302\n",
      "17     Civl Lib  -0.227222  -0.000000  -0.192869\n",
      "18        Hindu -11.708359  -0.000000  -0.568340\n",
      "19   Pr Exports   0.283370  -0.100853  -0.293213\n",
      "20   Pol Rights   0.122446  -0.000000  -0.045993\n",
      "21   R FEX Dist  -0.003287  -0.024223  -0.071190\n",
      "22          Age  -0.002263  -0.109492  -0.159086\n",
      "23    War Dummy  -0.280747  -0.000000  -0.133150\n",
      "24    English %  -0.910108  -0.000000  -0.176612\n",
      "25    Foreign %  -0.085370   0.000000  -0.051722\n",
      "26    Lab Force   0.044337   0.000000   0.370776\n",
      "27  Spanish Col   0.877837  -0.000000   0.193785\n",
      "28  EthnoL Frac   2.065396  -0.000000   0.328065\n",
      "29     std(BMP)  -0.018898  -0.041937  -0.068134\n",
      "30   French Col   1.006524  -0.000000   0.063420\n",
      "31      Abs Lat   0.015200   0.000000  -0.090777\n",
      "32     Work/Pop  -1.259016  -0.080610  -0.195259\n",
      "33  High Enroll -21.781882  -0.000000  -0.438741\n",
      "34        Pop g  -9.766829  -0.000000  -0.025944\n",
      "35     Brit Col   0.974357   0.000000   0.077573\n",
      "36    Outwar Or  -0.584359  -0.000000  -0.206428\n",
      "37       Jewish -24.629539  -0.000000   0.015170\n",
      "38   Rev & Coup   0.812733  -0.000000   0.073537\n",
      "39    %Publ Edu  22.026418   0.000000   0.159896\n",
      "40         Area  -0.026392  -0.000000   0.045055\n"
     ]
    }
   ],
   "source": [
    "print(coef_comp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94284fdf",
   "metadata": {},
   "source": [
    "22b. Write 2-3 sentences comparing the coefficients and explain why they are different.\n",
    "\n",
    "Answer here: The OLS coefficients tend to be a lot greater in magnitude as compared to the coefficients from the other two regressions. The Lasso regressions also have multiple variables with zero coefficients, which is not found in the other two regressions. This is because whilst OLS focuses solely on minimizing the sum of squared residuals, Lasso and Ridge regressions introduce penalty terms to control the size of the coefficients, leading to different coefficient estimates. Lasso tends to produce sparse models with some coefficients exactly zero, while Ridge shrinks coefficients towards zero but does not usually make them zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39410cdd",
   "metadata": {},
   "source": [
    "22c. Look at the Table 4 in Varian (2014) (link to paper in Canvas). Compare you coefficients from LASSO to the column he has for the LASSO regression. Do you come to the same conclusions about which regressors to keep in the regression and their order of importance? If they are different, explain a possible reason for that.\n",
    "\n",
    "Answer here: Yes, we came to the similar conclusions about which regressors to keep in the regression and their order of importance. The exception is the variable Buddha, which has a coefficient of 0.314402 in our regression, but is absent from Table 4, possibly because it did not have much significance in the other two regressions in the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c3068",
   "metadata": {},
   "source": [
    "# V. CLASSIFICATION: LOGIT vs. NEURAL NETWORK\n",
    "\n",
    "#### 23. Load the titanic3.csv file in as \"df_titanic\" from HW2_data.zip in the Canvas Assignment for HW2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73975686",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic = pd.read_csv('titanic3.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8622d",
   "metadata": {},
   "source": [
    "#### 24. In Varian (2014), \"Big Data: New Tricks for Econometrics\", there is a discussion about wanting to allow for nonlinearity in age to affect the prediction of survival of Titanic passengers. This problem will compare estimating a logit with estimating neural network (multilayer perceptron). Explain the following lines of code and run them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0d0624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71166718\n",
      "Iteration 2, loss = 0.69564375\n",
      "Iteration 3, loss = 0.68520857\n",
      "Iteration 4, loss = 0.67802560\n",
      "Iteration 5, loss = 0.67371194\n",
      "Iteration 6, loss = 0.67103982\n",
      "Iteration 7, loss = 0.66935104\n",
      "Iteration 8, loss = 0.66743090\n",
      "Iteration 9, loss = 0.66538807\n",
      "Iteration 10, loss = 0.66293164\n",
      "Iteration 11, loss = 0.66010487\n",
      "Iteration 12, loss = 0.65671367\n",
      "Iteration 13, loss = 0.65240347\n",
      "Iteration 14, loss = 0.64770667\n",
      "Iteration 15, loss = 0.64223625\n",
      "Iteration 16, loss = 0.63618211\n",
      "Iteration 17, loss = 0.62964568\n",
      "Iteration 18, loss = 0.62252273\n",
      "Iteration 19, loss = 0.61482248\n",
      "Iteration 20, loss = 0.60667495\n",
      "Iteration 21, loss = 0.59786557\n",
      "Iteration 22, loss = 0.58870687\n",
      "Iteration 23, loss = 0.57968533\n",
      "Iteration 24, loss = 0.57040901\n",
      "Iteration 25, loss = 0.56147817\n",
      "Iteration 26, loss = 0.55234323\n",
      "Iteration 27, loss = 0.54379160\n",
      "Iteration 28, loss = 0.53601911\n",
      "Iteration 29, loss = 0.52869627\n",
      "Iteration 30, loss = 0.52233033\n",
      "Iteration 31, loss = 0.51599733\n",
      "Iteration 32, loss = 0.51011936\n",
      "Iteration 33, loss = 0.50522173\n",
      "Iteration 34, loss = 0.50075591\n",
      "Iteration 35, loss = 0.49664485\n",
      "Iteration 36, loss = 0.49283276\n",
      "Iteration 37, loss = 0.48952047\n",
      "Iteration 38, loss = 0.48656204\n",
      "Iteration 39, loss = 0.48355458\n",
      "Iteration 40, loss = 0.48150140\n",
      "Iteration 41, loss = 0.47919972\n",
      "Iteration 42, loss = 0.47725189\n",
      "Iteration 43, loss = 0.47533007\n",
      "Iteration 44, loss = 0.47361908\n",
      "Iteration 45, loss = 0.47221271\n",
      "Iteration 46, loss = 0.47108516\n",
      "Iteration 47, loss = 0.46975546\n",
      "Iteration 48, loss = 0.46866720\n",
      "Iteration 49, loss = 0.46774065\n",
      "Iteration 50, loss = 0.46696209\n",
      "Iteration 51, loss = 0.46543506\n",
      "Iteration 52, loss = 0.46442761\n",
      "Iteration 53, loss = 0.46327225\n",
      "Iteration 54, loss = 0.46259162\n",
      "Iteration 55, loss = 0.46239550\n",
      "Iteration 56, loss = 0.46108839\n",
      "Iteration 57, loss = 0.46041167\n",
      "Iteration 58, loss = 0.45981930\n",
      "Iteration 59, loss = 0.45935899\n",
      "Iteration 60, loss = 0.45879618\n",
      "Iteration 61, loss = 0.45814538\n",
      "Iteration 62, loss = 0.45757360\n",
      "Iteration 63, loss = 0.45668898\n",
      "Iteration 64, loss = 0.45643084\n",
      "Iteration 65, loss = 0.45604896\n",
      "Iteration 66, loss = 0.45555667\n",
      "Iteration 67, loss = 0.45498798\n",
      "Iteration 68, loss = 0.45465742\n",
      "Iteration 69, loss = 0.45389672\n",
      "Iteration 70, loss = 0.45400424\n",
      "Iteration 71, loss = 0.45339990\n",
      "Iteration 72, loss = 0.45294198\n",
      "Iteration 73, loss = 0.45258002\n",
      "Iteration 74, loss = 0.45225680\n",
      "Iteration 75, loss = 0.45146065\n",
      "Iteration 76, loss = 0.45077920\n",
      "Iteration 77, loss = 0.45073901\n",
      "Iteration 78, loss = 0.45035627\n",
      "Iteration 79, loss = 0.45016523\n",
      "Iteration 80, loss = 0.44973614\n",
      "Iteration 81, loss = 0.44932752\n",
      "Iteration 82, loss = 0.44890936\n",
      "Iteration 83, loss = 0.44868632\n",
      "Iteration 84, loss = 0.44825281\n",
      "Iteration 85, loss = 0.44782804\n",
      "Iteration 86, loss = 0.44750819\n",
      "Iteration 87, loss = 0.44807662\n",
      "Iteration 88, loss = 0.44817499\n",
      "Iteration 89, loss = 0.44776845\n",
      "Iteration 90, loss = 0.44689111\n",
      "Iteration 91, loss = 0.44629861\n",
      "Iteration 92, loss = 0.44605975\n",
      "Iteration 93, loss = 0.44598846\n",
      "Iteration 94, loss = 0.44601327\n",
      "Iteration 95, loss = 0.44608612\n",
      "Iteration 96, loss = 0.44580171\n",
      "Iteration 97, loss = 0.44520288\n",
      "Iteration 98, loss = 0.44467889\n",
      "Iteration 99, loss = 0.44483546\n",
      "Iteration 100, loss = 0.44494537\n",
      "Iteration 101, loss = 0.44478049\n",
      "Iteration 102, loss = 0.44429273\n",
      "Iteration 103, loss = 0.44415379\n",
      "Iteration 104, loss = 0.44353009\n",
      "Iteration 105, loss = 0.44343126\n",
      "Iteration 106, loss = 0.44336633\n",
      "Iteration 107, loss = 0.44375810\n",
      "Iteration 108, loss = 0.44308108\n",
      "Iteration 109, loss = 0.44302754\n",
      "Iteration 110, loss = 0.44269672\n",
      "Iteration 111, loss = 0.44274081\n",
      "Iteration 112, loss = 0.44293671\n",
      "Iteration 113, loss = 0.44265968\n",
      "Iteration 114, loss = 0.44245009\n",
      "Iteration 115, loss = 0.44237114\n",
      "Iteration 116, loss = 0.44206141\n",
      "Iteration 117, loss = 0.44210746\n",
      "Iteration 118, loss = 0.44212929\n",
      "Iteration 119, loss = 0.44173635\n",
      "Iteration 120, loss = 0.44172917\n",
      "Iteration 121, loss = 0.44150225\n",
      "Iteration 122, loss = 0.44146846\n",
      "Iteration 123, loss = 0.44137832\n",
      "Iteration 124, loss = 0.44132983\n",
      "Iteration 125, loss = 0.44114690\n",
      "Iteration 126, loss = 0.44100544\n",
      "Iteration 127, loss = 0.44188374\n",
      "Iteration 128, loss = 0.44116454\n",
      "Iteration 129, loss = 0.44098902\n",
      "Iteration 130, loss = 0.44116801\n",
      "Iteration 131, loss = 0.44085864\n",
      "Iteration 132, loss = 0.44091981\n",
      "Iteration 133, loss = 0.44049528\n",
      "Iteration 134, loss = 0.44053138\n",
      "Iteration 135, loss = 0.44053738\n",
      "Iteration 136, loss = 0.44033819\n",
      "Iteration 137, loss = 0.44024998\n",
      "Iteration 138, loss = 0.44016534\n",
      "Iteration 139, loss = 0.44010142\n",
      "Iteration 140, loss = 0.44028315\n",
      "Iteration 141, loss = 0.44021719\n",
      "Iteration 142, loss = 0.43998023\n",
      "Iteration 143, loss = 0.43974332\n",
      "Iteration 144, loss = 0.43971696\n",
      "Iteration 145, loss = 0.43971643\n",
      "Iteration 146, loss = 0.43953876\n",
      "Iteration 147, loss = 0.43957465\n",
      "Iteration 148, loss = 0.43982144\n",
      "Iteration 149, loss = 0.44006412\n",
      "Iteration 150, loss = 0.43938363\n",
      "Iteration 151, loss = 0.43938990\n",
      "Iteration 152, loss = 0.43941845\n",
      "Iteration 153, loss = 0.43934152\n",
      "Iteration 154, loss = 0.43950925\n",
      "Iteration 155, loss = 0.43920843\n",
      "Iteration 156, loss = 0.43904215\n",
      "Iteration 157, loss = 0.43907895\n",
      "Iteration 158, loss = 0.43889633\n",
      "Iteration 159, loss = 0.43877681\n",
      "Iteration 160, loss = 0.43871036\n",
      "Iteration 161, loss = 0.43871964\n",
      "Iteration 162, loss = 0.43856255\n",
      "Iteration 163, loss = 0.43866152\n",
      "Iteration 164, loss = 0.43863636\n",
      "Iteration 165, loss = 0.43858604\n",
      "Iteration 166, loss = 0.43849784\n",
      "Iteration 167, loss = 0.43868626\n",
      "Iteration 168, loss = 0.43871881\n",
      "Iteration 169, loss = 0.43838606\n",
      "Iteration 170, loss = 0.43830630\n",
      "Iteration 171, loss = 0.43832139\n",
      "Iteration 172, loss = 0.43824694\n",
      "Iteration 173, loss = 0.43819155\n",
      "Iteration 174, loss = 0.43819710\n",
      "Iteration 175, loss = 0.43904735\n",
      "Iteration 176, loss = 0.43926521\n",
      "Iteration 177, loss = 0.43832611\n",
      "Iteration 178, loss = 0.43816531\n",
      "Iteration 179, loss = 0.43798871\n",
      "Iteration 180, loss = 0.43816875\n",
      "Iteration 181, loss = 0.43813913\n",
      "Iteration 182, loss = 0.43819705\n",
      "Iteration 183, loss = 0.43782033\n",
      "Iteration 184, loss = 0.43774545\n",
      "Iteration 185, loss = 0.43780127\n",
      "Iteration 186, loss = 0.43764685\n",
      "Iteration 187, loss = 0.43770271\n",
      "Iteration 188, loss = 0.43768801\n",
      "Iteration 189, loss = 0.43747252\n",
      "Iteration 190, loss = 0.43778345\n",
      "Iteration 191, loss = 0.43776920\n",
      "Iteration 192, loss = 0.43783604\n",
      "Iteration 193, loss = 0.43757698\n",
      "Iteration 194, loss = 0.43753950\n",
      "Iteration 195, loss = 0.43766676\n",
      "Iteration 196, loss = 0.43742867\n",
      "Iteration 197, loss = 0.43769370\n",
      "Iteration 198, loss = 0.43698000\n",
      "Iteration 199, loss = 0.43755444\n",
      "Iteration 200, loss = 0.43886079\n",
      "Iteration 201, loss = 0.43857381\n",
      "Iteration 202, loss = 0.43775442\n",
      "Iteration 203, loss = 0.43736451\n",
      "Iteration 204, loss = 0.43713683\n",
      "Iteration 205, loss = 0.43761689\n",
      "Iteration 206, loss = 0.43695774\n",
      "Iteration 207, loss = 0.43717902\n",
      "Iteration 208, loss = 0.43735451\n",
      "Iteration 209, loss = 0.43767873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP Training Accuracy: 0.8038277511961722\n",
      "MLP Testing Accuracy: 0.7952380952380952\n",
      "Logit Training Accuracy: 0.7882775119617225\n",
      "Logit Testing Accuracy: 0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "df_titanic.dropna(subset=['survived','age', 'sex','pclass'],inplace=True) \n",
    "# Create a female dummy variable \n",
    "df_titanic['female']= np.where(df_titanic['sex']=='female',1,0) \n",
    "# Assign input and output variables, split data into train and test \n",
    "y= df_titanic['survived']\n",
    "X= df_titanic[['age','pclass', 'female']] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1680) \n",
    "\n",
    "# Standardize the features\n",
    "scaler=StandardScaler() \n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# train the multilayer perceptron classifier \n",
    "MLP = MLPClassifier(hidden_layer_sizes=(4,2),\n",
    "  random_state=1680,\n",
    "                    activation='logistic', solver='adam', \n",
    "                    max_iter =500,\n",
    "                    verbose=True, learning_rate_init=0.01) \n",
    "MLP.fit(X_train,y_train)\n",
    "print(\"MLP Training Accuracy:\",accuracy_score(y_train,MLP.predict(X_train)))\n",
    "print(\"MLP Testing Accuracy:\",accuracy_score(y_test, MLP.predict(X_test)))\n",
    "\n",
    "# train the logistic regression model \n",
    "logitmodel = LogisticRegression(solver='liblinear', random_state=1680).fit(X_train, y_train) \n",
    "print(\"Logit Training Accuracy:\", accuracy_score(y_train,logitmodel.predict(X_train)))\n",
    "print(\"Logit Testing Accuracy:\", accuracy_score(y_test, logitmodel.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9641e347",
   "metadata": {},
   "source": [
    "24a. Compare the accuracy scores in-sample and out-of-sample for the logit regression and for the neural network in 2-3 sentences. \n",
    "\n",
    "Answer here: The neural network performed better both in-sample (training accuracy) and out of sample (testing accuracy). However, the differences were not extremely noticable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae2e97e",
   "metadata": {},
   "source": [
    "24b. Is the difference between the accuracies what you would have expected? Spend 2-3 sentences discussing possible explanations for the differences. \n",
    "\n",
    "Answer here: These differences in accuracies are expected, since a linear model like logistic regression is likely unable to discover more nuances in the data, as the paper explains. However, it is interesting that the gap in accuracy scores is not as big as expected, showing that even if logistic regression may not uncover all hidden trends, it is still a relatively powerful predictor. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7101d",
   "metadata": {},
   "source": [
    "24c. Print out the descriptive statistics for y_train and y_test\n",
    "\n",
    "Paste output here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896ff243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for y_train:\n",
      "count    836.000000\n",
      "mean       0.399522\n",
      "std        0.490093\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: survived, dtype: float64\n",
      "\n",
      "Descriptive Statistics for y_test:\n",
      "count    210.000000\n",
      "mean       0.442857\n",
      "std        0.497911\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Descriptive Statistics for y_train:\")\n",
    "print(y_train.describe())\n",
    "\n",
    "print(\"\\nDescriptive Statistics for y_test:\")\n",
    "print(y_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86368c1e",
   "metadata": {},
   "source": [
    "#### 25. Now rerun the previous code, but add stratify=y into the train_test_split command.\n",
    "\n",
    "25a. How do the descriptive statistics for y_train and y_test change and how do the accuracy scores change?\n",
    "\n",
    "Answer here: The accuracy score for logistic regression decreased for the test dataset, whereas the MLP network accuracy remained fairly constant. The mean and std for y_train and y_test is now more evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291119ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70894753\n",
      "Iteration 2, loss = 0.69741544\n",
      "Iteration 3, loss = 0.68669943\n",
      "Iteration 4, loss = 0.68101162\n",
      "Iteration 5, loss = 0.67693919\n",
      "Iteration 6, loss = 0.67398654\n",
      "Iteration 7, loss = 0.67215091\n",
      "Iteration 8, loss = 0.67096901\n",
      "Iteration 9, loss = 0.66898181\n",
      "Iteration 10, loss = 0.66659063\n",
      "Iteration 11, loss = 0.66320451\n",
      "Iteration 12, loss = 0.65927254\n",
      "Iteration 13, loss = 0.65449687\n",
      "Iteration 14, loss = 0.64898397\n",
      "Iteration 15, loss = 0.64192832\n",
      "Iteration 16, loss = 0.63462357\n",
      "Iteration 17, loss = 0.62531451\n",
      "Iteration 18, loss = 0.61571887\n",
      "Iteration 19, loss = 0.60527922\n",
      "Iteration 20, loss = 0.59445752\n",
      "Iteration 21, loss = 0.58344477\n",
      "Iteration 22, loss = 0.57183734\n",
      "Iteration 23, loss = 0.56036779\n",
      "Iteration 24, loss = 0.54944270\n",
      "Iteration 25, loss = 0.53896643\n",
      "Iteration 26, loss = 0.52987271\n",
      "Iteration 27, loss = 0.52153519\n",
      "Iteration 28, loss = 0.51403032\n",
      "Iteration 29, loss = 0.50749977\n",
      "Iteration 30, loss = 0.50196190\n",
      "Iteration 31, loss = 0.49711786\n",
      "Iteration 32, loss = 0.49308008\n",
      "Iteration 33, loss = 0.48970534\n",
      "Iteration 34, loss = 0.48651464\n",
      "Iteration 35, loss = 0.48344136\n",
      "Iteration 36, loss = 0.48084612\n",
      "Iteration 37, loss = 0.47854965\n",
      "Iteration 38, loss = 0.47662251\n",
      "Iteration 39, loss = 0.47484221\n",
      "Iteration 40, loss = 0.47290417\n",
      "Iteration 41, loss = 0.47148746\n",
      "Iteration 42, loss = 0.47022571\n",
      "Iteration 43, loss = 0.46899535\n",
      "Iteration 44, loss = 0.46797152\n",
      "Iteration 45, loss = 0.46716896\n",
      "Iteration 46, loss = 0.46607139\n",
      "Iteration 47, loss = 0.46496629\n",
      "Iteration 48, loss = 0.46448952\n",
      "Iteration 49, loss = 0.46318059\n",
      "Iteration 50, loss = 0.46232900\n",
      "Iteration 51, loss = 0.46160886\n",
      "Iteration 52, loss = 0.46081428\n",
      "Iteration 53, loss = 0.46001537\n",
      "Iteration 54, loss = 0.45954855\n",
      "Iteration 55, loss = 0.45907291\n",
      "Iteration 56, loss = 0.45810280\n",
      "Iteration 57, loss = 0.45773429\n",
      "Iteration 58, loss = 0.45735887\n",
      "Iteration 59, loss = 0.45662992\n",
      "Iteration 60, loss = 0.45569166\n",
      "Iteration 61, loss = 0.45501053\n",
      "Iteration 62, loss = 0.45459642\n",
      "Iteration 63, loss = 0.45432366\n",
      "Iteration 64, loss = 0.45357091\n",
      "Iteration 65, loss = 0.45319200\n",
      "Iteration 66, loss = 0.45247689\n",
      "Iteration 67, loss = 0.45292822\n",
      "Iteration 68, loss = 0.45337949\n",
      "Iteration 69, loss = 0.45280203\n",
      "Iteration 70, loss = 0.45199738\n",
      "Iteration 71, loss = 0.45073036\n",
      "Iteration 72, loss = 0.45009326\n",
      "Iteration 73, loss = 0.44958757\n",
      "Iteration 74, loss = 0.44964337\n",
      "Iteration 75, loss = 0.44940204\n",
      "Iteration 76, loss = 0.44922017\n",
      "Iteration 77, loss = 0.44951484\n",
      "Iteration 78, loss = 0.44864363\n",
      "Iteration 79, loss = 0.44801162\n",
      "Iteration 80, loss = 0.44782928\n",
      "Iteration 81, loss = 0.44754956\n",
      "Iteration 82, loss = 0.44737481\n",
      "Iteration 83, loss = 0.44720824\n",
      "Iteration 84, loss = 0.44651266\n",
      "Iteration 85, loss = 0.44617380\n",
      "Iteration 86, loss = 0.44599485\n",
      "Iteration 87, loss = 0.44566158\n",
      "Iteration 88, loss = 0.44544934\n",
      "Iteration 89, loss = 0.44525506\n",
      "Iteration 90, loss = 0.44503439\n",
      "Iteration 91, loss = 0.44460125\n",
      "Iteration 92, loss = 0.44442496\n",
      "Iteration 93, loss = 0.44427671\n",
      "Iteration 94, loss = 0.44410551\n",
      "Iteration 95, loss = 0.44448040\n",
      "Iteration 96, loss = 0.44416481\n",
      "Iteration 97, loss = 0.44382579\n",
      "Iteration 98, loss = 0.44352747\n",
      "Iteration 99, loss = 0.44332383\n",
      "Iteration 100, loss = 0.44311279\n",
      "Iteration 101, loss = 0.44302381\n",
      "Iteration 102, loss = 0.44299839\n",
      "Iteration 103, loss = 0.44350262\n",
      "Iteration 104, loss = 0.44320671\n",
      "Iteration 105, loss = 0.44324638\n",
      "Iteration 106, loss = 0.44170535\n",
      "Iteration 107, loss = 0.44165513\n",
      "Iteration 108, loss = 0.44167488\n",
      "Iteration 109, loss = 0.44133792\n",
      "Iteration 110, loss = 0.44116415\n",
      "Iteration 111, loss = 0.44159924\n",
      "Iteration 112, loss = 0.44169044\n",
      "Iteration 113, loss = 0.44122010\n",
      "Iteration 114, loss = 0.44054329\n",
      "Iteration 115, loss = 0.44028948\n",
      "Iteration 116, loss = 0.43990991\n",
      "Iteration 117, loss = 0.44016751\n",
      "Iteration 118, loss = 0.43978895\n",
      "Iteration 119, loss = 0.43923108\n",
      "Iteration 120, loss = 0.43943849\n",
      "Iteration 121, loss = 0.43963478\n",
      "Iteration 122, loss = 0.43990661\n",
      "Iteration 123, loss = 0.43948293\n",
      "Iteration 124, loss = 0.43876472\n",
      "Iteration 125, loss = 0.43849687\n",
      "Iteration 126, loss = 0.43845536\n",
      "Iteration 127, loss = 0.43833323\n",
      "Iteration 128, loss = 0.43883088\n",
      "Iteration 129, loss = 0.43808958\n",
      "Iteration 130, loss = 0.43754464\n",
      "Iteration 131, loss = 0.43706371\n",
      "Iteration 132, loss = 0.43681648\n",
      "Iteration 133, loss = 0.43687862\n",
      "Iteration 134, loss = 0.43667614\n",
      "Iteration 135, loss = 0.43649638\n",
      "Iteration 136, loss = 0.43607459\n",
      "Iteration 137, loss = 0.43577466\n",
      "Iteration 138, loss = 0.43552242\n",
      "Iteration 139, loss = 0.43521842\n",
      "Iteration 140, loss = 0.43572647\n",
      "Iteration 141, loss = 0.43583102\n",
      "Iteration 142, loss = 0.43576446\n",
      "Iteration 143, loss = 0.43480281\n",
      "Iteration 144, loss = 0.43481371\n",
      "Iteration 145, loss = 0.43437316\n",
      "Iteration 146, loss = 0.43427433\n",
      "Iteration 147, loss = 0.43436273\n",
      "Iteration 148, loss = 0.43456451\n",
      "Iteration 149, loss = 0.43388004\n",
      "Iteration 150, loss = 0.43296748\n",
      "Iteration 151, loss = 0.43312479\n",
      "Iteration 152, loss = 0.43414538\n",
      "Iteration 153, loss = 0.43433527\n",
      "Iteration 154, loss = 0.43382614\n",
      "Iteration 155, loss = 0.43285482\n",
      "Iteration 156, loss = 0.43240875\n",
      "Iteration 157, loss = 0.43259600\n",
      "Iteration 158, loss = 0.43230690\n",
      "Iteration 159, loss = 0.43193629\n",
      "Iteration 160, loss = 0.43204670\n",
      "Iteration 161, loss = 0.43181159\n",
      "Iteration 162, loss = 0.43197666\n",
      "Iteration 163, loss = 0.43148609\n",
      "Iteration 164, loss = 0.43083452\n",
      "Iteration 165, loss = 0.43070318\n",
      "Iteration 166, loss = 0.43060605\n",
      "Iteration 167, loss = 0.43052166\n",
      "Iteration 168, loss = 0.43036037\n",
      "Iteration 169, loss = 0.43032264\n",
      "Iteration 170, loss = 0.43021488\n",
      "Iteration 171, loss = 0.43004933\n",
      "Iteration 172, loss = 0.43007661\n",
      "Iteration 173, loss = 0.43014175\n",
      "Iteration 174, loss = 0.43004125\n",
      "Iteration 175, loss = 0.42982602\n",
      "Iteration 176, loss = 0.42983218\n",
      "Iteration 177, loss = 0.42954329\n",
      "Iteration 178, loss = 0.42920220\n",
      "Iteration 179, loss = 0.42896831\n",
      "Iteration 180, loss = 0.42871912\n",
      "Iteration 181, loss = 0.42875433\n",
      "Iteration 182, loss = 0.42866140\n",
      "Iteration 183, loss = 0.42917229\n",
      "Iteration 184, loss = 0.42934972\n",
      "Iteration 185, loss = 0.42953907\n",
      "Iteration 186, loss = 0.42974757\n",
      "Iteration 187, loss = 0.43042244\n",
      "Iteration 188, loss = 0.42969124\n",
      "Iteration 189, loss = 0.42875240\n",
      "Iteration 190, loss = 0.42800716\n",
      "Iteration 191, loss = 0.42759129\n",
      "Iteration 192, loss = 0.42786322\n",
      "Iteration 193, loss = 0.42742346\n",
      "Iteration 194, loss = 0.42772104\n",
      "Iteration 195, loss = 0.42745466\n",
      "Iteration 196, loss = 0.42724313\n",
      "Iteration 197, loss = 0.42718933\n",
      "Iteration 198, loss = 0.42700708\n",
      "Iteration 199, loss = 0.42731125\n",
      "Iteration 200, loss = 0.42717705\n",
      "Iteration 201, loss = 0.42717082\n",
      "Iteration 202, loss = 0.42695755\n",
      "Iteration 203, loss = 0.42683940\n",
      "Iteration 204, loss = 0.42669106\n",
      "Iteration 205, loss = 0.42688594\n",
      "Iteration 206, loss = 0.42673341\n",
      "Iteration 207, loss = 0.42669542\n",
      "Iteration 208, loss = 0.42693887\n",
      "Iteration 209, loss = 0.42664727\n",
      "Iteration 210, loss = 0.42649493\n",
      "Iteration 211, loss = 0.42621209\n",
      "Iteration 212, loss = 0.42750075\n",
      "Iteration 213, loss = 0.42816448\n",
      "Iteration 214, loss = 0.42686658\n",
      "Iteration 215, loss = 0.42604203\n",
      "Iteration 216, loss = 0.42676958\n",
      "Iteration 217, loss = 0.42670164\n",
      "Iteration 218, loss = 0.42655304\n",
      "Iteration 219, loss = 0.42612958\n",
      "Iteration 220, loss = 0.42638685\n",
      "Iteration 221, loss = 0.42622857\n",
      "Iteration 222, loss = 0.42595935\n",
      "Iteration 223, loss = 0.42574123\n",
      "Iteration 224, loss = 0.42575017\n",
      "Iteration 225, loss = 0.42581423\n",
      "Iteration 226, loss = 0.42583776\n",
      "Iteration 227, loss = 0.42600166\n",
      "Iteration 228, loss = 0.42580239\n",
      "Iteration 229, loss = 0.42569681\n",
      "Iteration 230, loss = 0.42568530\n",
      "Iteration 231, loss = 0.42556476\n",
      "Iteration 232, loss = 0.42546466\n",
      "Iteration 233, loss = 0.42542519\n",
      "Iteration 234, loss = 0.42582979\n",
      "Iteration 235, loss = 0.42631157\n",
      "Iteration 236, loss = 0.42625451\n",
      "Iteration 237, loss = 0.42558842\n",
      "Iteration 238, loss = 0.42542647\n",
      "Iteration 239, loss = 0.42559623\n",
      "Iteration 240, loss = 0.42551892\n",
      "Iteration 241, loss = 0.42536390\n",
      "Iteration 242, loss = 0.42534441\n",
      "Iteration 243, loss = 0.42534549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP Training Accuracy: 0.8026315789473685\n",
      "MLP Testing Accuracy: 0.7952380952380952\n",
      "Logit Training Accuracy: 0.7954545454545454\n",
      "Logit Testing Accuracy: 0.7571428571428571\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1680, stratify=y) \n",
    "\n",
    "# Standardize the features\n",
    "scaler=StandardScaler() \n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# train the multilayer perceptron classifier \n",
    "MLP = MLPClassifier(hidden_layer_sizes=(4,2),\n",
    "  random_state=1680,\n",
    "                    activation='logistic', solver='adam', \n",
    "                    max_iter =500,\n",
    "                    verbose=True, learning_rate_init=0.01) \n",
    "MLP.fit(X_train,y_train)\n",
    "print(\"MLP Training Accuracy:\",accuracy_score(y_train,MLP.predict(X_train)))\n",
    "print(\"MLP Testing Accuracy:\",accuracy_score(y_test, MLP.predict(X_test)))\n",
    "\n",
    "# train the logistic regression model \n",
    "logitmodel = LogisticRegression(solver='liblinear', random_state=1680).fit(X_train, y_train) \n",
    "print(\"Logit Training Accuracy:\", accuracy_score(y_train,logitmodel.predict(X_train)))\n",
    "print(\"Logit Testing Accuracy:\", accuracy_score(y_test, logitmodel.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764d9d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for y_train:\n",
      "count    836.000000\n",
      "mean       0.407895\n",
      "std        0.491738\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: survived, dtype: float64\n",
      "\n",
      "Descriptive Statistics for y_test:\n",
      "count    210.000000\n",
      "mean       0.409524\n",
      "std        0.492921\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Descriptive Statistics for y_train:\")\n",
    "print(y_train.describe())\n",
    "\n",
    "print(\"\\nDescriptive Statistics for y_test:\")\n",
    "print(y_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f274b",
   "metadata": {},
   "source": [
    "25b. What does “stratify” do and why would it change your results?\n",
    "\n",
    "Answer here:  Stratify makes the y_train and y_test datasets more similar in distribution in terms of y, which ensures that the model is trained and evaluated on representative samples of each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d0460",
   "metadata": {},
   "source": [
    "#### 26. Look at the other variables in df_titanic. What other variables besides age do you think would be important in predicting survival?\n",
    "\n",
    "26a. List the variables you think of as important here and explain why you think they would improve prediction. Hint: you may need to transform variables into numerical representations/dummy variables.\n",
    "\n",
    "Answer here: In addition to age, sex and class, I chose parch (number of parents and children) since I suspect having family on board may slow down evacuation. Furthermore, I chose embarked to capture geographical and cultural differences between individuals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6346faf",
   "metadata": {},
   "source": [
    "26b. Run the logit regression with the variables that you listed and print the accuracy below. Did your accuracy improve?\n",
    "\n",
    "Answer here: We can see that the training accuracy decreased slightly, but the testing accuarcy increased slightly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9e80d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit Training Accuracy: 0.7892215568862275\n",
      "Logit Testing Accuracy: 0.7799043062200957\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "df_titanic.dropna(subset=['survived','age', 'sex','pclass', 'parch', 'embarked'],inplace=True) \n",
    "# Create a female dummy variable \n",
    "df_titanic['female']= np.where(df_titanic['sex']=='female',1,0) \n",
    "# Create a has family dummy variable \n",
    "df_titanic['family']= np.where(df_titanic['parch']==0,0,1)\n",
    "# Create dummies for each port of embarkation \n",
    "df_titanic['Cherbourg']= np.where(df_titanic['embarked']=='C',1,0)\n",
    "df_titanic['Queenstown']= np.where(df_titanic['embarked']=='Q',1,0)\n",
    "df_titanic['Southampton']= np.where(df_titanic['embarked']=='S',1,0)\n",
    "\n",
    "# Assign input and output variables, split data into train and test \n",
    "y= df_titanic['survived']\n",
    "X= df_titanic[['age','pclass', 'female', 'family', 'Cherbourg', 'Queenstown', 'Southampton']] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1680, stratify=y) \n",
    "\n",
    "# Standardize the features\n",
    "scaler=StandardScaler() \n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# train the logistic regression model \n",
    "logitmodel = LogisticRegression(solver='liblinear', random_state=1680).fit(X_train, y_train) \n",
    "print(\"Logit Training Accuracy:\", accuracy_score(y_train,logitmodel.predict(X_train)))\n",
    "print(\"Logit Testing Accuracy:\", accuracy_score(y_test, logitmodel.predict(X_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff6ebcde2956354653a1bfc837161093b4abe74b5e977ba193ed76204ade37d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
