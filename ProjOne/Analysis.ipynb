{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0730ea-1316-4f07-8457-825283980e95",
   "metadata": {},
   "source": [
    "# Project One Code \n",
    "\n",
    "## Econ 1680: MLTA and Econ\n",
    "\n",
    "#### Name: Nadya Tan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af5d5c61-dff6-4e56-8811-10646aef170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba512fce",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01d69b1-a1bc-4ba4-bdd6-a24759329f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microdata = pd.read_csv('micro_world_139countries.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88f74001-1ef2-4466-a3f0-b412e905d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns to keep \n",
    "selected = ['economycode', 'wgt','female', 'age', 'educ', 'inc_q', 'emp_in', 'urbanicity_f2f', 'account', 'account_fin', 'account_mob', 'fin2', 'fin4', 'fin5', 'fin6', 'fin7', 'fin8', 'fin9', 'fin10', 'fin13a', 'saved', 'borrowed', 'receive_wages', 'receive_transfers', 'receive_pension', 'receive_agriculture', 'pay_utilities', 'remittances', 'mobileowner', 'internetaccess', 'anydigpayment', 'merchantpay_dig' ]\n",
    "# new df \n",
    "df_selected = df_microdata[selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7967c0a8-d8d2-4426-b181-9b7a1b955d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143887, 32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ded9a-1c3b-4349-83f6-5ae5f7eb827e",
   "metadata": {},
   "source": [
    "## One-hot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eec12546-ddc3-426e-967a-3881d27cb816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these variables need maps\n",
    "# educ - change to 3 columns with 0-1 - if value is 4-5, put na in all 3 cols \n",
    "educ_dummies = pd.get_dummies(df_selected['educ'], prefix='educ').astype(int)\n",
    "educ_dummies = educ_dummies.drop(columns=['educ_4', 'educ_5'])\n",
    "# Fill NaN values for rows where educ was 4 or 5\n",
    "educ_dummies.loc[df_selected['educ'].isin([4, 5]), :] = np.nan\n",
    "\n",
    "# Convert inc_q to 5 columns\n",
    "inc_q_dummies = pd.get_dummies(df_selected['inc_q'], prefix='inc_q').astype(int)\n",
    "df_selected = pd.concat([df_selected, educ_dummies, inc_q_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d23b68cd-997e-4a99-9cc6-032b8320393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert female to 0-1\n",
    "df_selected['female'] = df_selected['female'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Convert emp_in to 0-1\n",
    "df_selected['emp_in'] = df_selected['emp_in'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Convert urbanicity_f2f to 0-1\n",
    "df_selected['urbanicity_f2f'] = df_selected['urbanicity_f2f'].apply(lambda x: 1 if x == 2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f3904fa-ee33-497b-a246-0cf198dfebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert these other financial variables to 0-1 and na for 3-4 \n",
    "other_cols = [col for col in df_selected.columns if col in ['fin2', 'fin4', 'fin5', 'fin6', 'fin7', 'fin8', 'fin9', 'fin10', 'fin13a','mobileowner', 'internetaccess']]\n",
    "for col in other_cols:\n",
    "    df_selected[col] = df_selected[col].apply(lambda x: 1 if x == 1 else (0 if x == 2 else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ddec655-7142-4ab7-becb-0a547ae9f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_value(value):\n",
    "    # did not perform the act\n",
    "    if value == 4:\n",
    "        return 0\n",
    "    # performed using cash\n",
    "    elif value == 2:\n",
    "        return 1\n",
    "    # performed using account \n",
    "    elif value == 1:\n",
    "        return 2\n",
    "    else:\n",
    "        return np.nan \n",
    "\n",
    "# Create new columns with scaled values\n",
    "df_selected['receive_wages_scaled'] = df_selected['receive_wages'].apply(scale_value)\n",
    "df_selected['receive_transfers_scaled'] = df_selected['receive_transfers'].apply(scale_value)\n",
    "df_selected['receive_pension_scaled'] = df_selected['receive_pension'].apply(scale_value)\n",
    "df_selected['receive_agriculture_scaled'] = df_selected['receive_agriculture'].apply(scale_value)\n",
    "df_selected['pay_utilities_scaled'] = df_selected['pay_utilities'].apply(scale_value)\n",
    "\n",
    "# Replace old column with 0-1 \n",
    "def replace_value(value):\n",
    "    if value in [1, 2, 3]:\n",
    "        return 1\n",
    "    elif value == 4:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan  # NaN for values 5\n",
    "columns_to_process = ['receive_wages', 'receive_transfers', 'receive_pension', 'receive_agriculture', 'pay_utilities']\n",
    "for column in columns_to_process:\n",
    "    df_selected[column] = df_selected[column].apply(replace_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "05c729e5-27fd-45d9-b38e-46d0dddefee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_value_remittances(value):\n",
    "    # did not perform the act\n",
    "    if value == 5:\n",
    "        return 0\n",
    "    # performed using cash or OTC\n",
    "    elif value in [2, 3]:\n",
    "        return 1\n",
    "    # performed using account \n",
    "    elif value == 1:\n",
    "        return 2\n",
    "    else:\n",
    "        return np.nan \n",
    "df_selected['remittances_scaled'] = df_selected['remittances'].apply(scale_value)\n",
    "\n",
    "def replace_value_remittances(value):\n",
    "    if value in [1, 2, 3, 4]:\n",
    "        return 1\n",
    "    elif value == 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df_selected['remittances'] = df_selected['remittances'].apply(replace_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cbcdbe6d-8937-4f2e-8c74-70c4729fffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# economycode \n",
    "economycode_dummies = pd.get_dummies(df_selected['economycode']).astype(int)\n",
    "df_selected = pd.concat([df_selected, economycode_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "02afc3e0-e82f-42ea-8a03-a5432edc0ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143887, 185)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f21ee8af-7290-4af2-bdae-48a8efa9d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all NaNs - NOTE: SHOULD DO THIS AFTER SUMMARY STATS \n",
    "columns_to_process = ['economycode', 'wgt','female', 'age', 'educ', 'inc_q', 'emp_in', 'urbanicity_f2f', 'account', 'account_fin', 'account_mob',  'fin13a', 'saved', 'borrowed', 'receive_wages', 'receive_transfers', 'receive_pension', 'receive_agriculture', 'pay_utilities', 'remittances', 'mobileowner', 'internetaccess', 'anydigpayment', 'merchantpay_dig','receive_wages_scaled', 'receive_transfers_scaled','receive_pension_scaled','receive_agriculture_scaled','pay_utilities_scaled', 'remittances_scaled'  ]\n",
    "\n",
    "# Drop rows where any value in the specified columns is NaN\n",
    "df_selected = df_selected.dropna(subset=columns_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "16ee1c90-da73-4ddf-bcbc-d11f01c85f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11448, 185)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5dd885db-5587-46db-8c1b-57db73983420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.to_csv('microdata_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e32308f-c078-4606-890a-bc5ad0f060f9",
   "metadata": {},
   "source": [
    "## Summary Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0adef670-c421-4e68-b79a-569695139598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>inc_q</th>\n",
       "      <th>emp_in</th>\n",
       "      <th>urbanicity_f2f</th>\n",
       "      <th>account</th>\n",
       "      <th>account_fin</th>\n",
       "      <th>account_mob</th>\n",
       "      <th>saved</th>\n",
       "      <th>borrowed</th>\n",
       "      <th>receive_wages</th>\n",
       "      <th>receive_transfers</th>\n",
       "      <th>receive_pension</th>\n",
       "      <th>receive_agriculture</th>\n",
       "      <th>pay_utilities</th>\n",
       "      <th>remittances</th>\n",
       "      <th>mobileowner</th>\n",
       "      <th>internetaccess</th>\n",
       "      <th>anydigpayment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.0</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.0</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.465671</td>\n",
       "      <td>32.895790</td>\n",
       "      <td>1.993623</td>\n",
       "      <td>3.759085</td>\n",
       "      <td>0.818833</td>\n",
       "      <td>0.478424</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.627446</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.787037</td>\n",
       "      <td>0.710255</td>\n",
       "      <td>0.449773</td>\n",
       "      <td>0.149371</td>\n",
       "      <td>0.046209</td>\n",
       "      <td>0.196716</td>\n",
       "      <td>0.542628</td>\n",
       "      <td>0.962002</td>\n",
       "      <td>0.959644</td>\n",
       "      <td>0.754630</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498842</td>\n",
       "      <td>12.106833</td>\n",
       "      <td>0.640126</td>\n",
       "      <td>1.318487</td>\n",
       "      <td>0.385173</td>\n",
       "      <td>0.499556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409420</td>\n",
       "      <td>0.453664</td>\n",
       "      <td>0.497493</td>\n",
       "      <td>0.356469</td>\n",
       "      <td>0.209946</td>\n",
       "      <td>0.397533</td>\n",
       "      <td>0.498201</td>\n",
       "      <td>0.191200</td>\n",
       "      <td>0.196802</td>\n",
       "      <td>0.430325</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             female           age          educ         inc_q        emp_in  \\\n",
       "count  11448.000000  11448.000000  11448.000000  11448.000000  11448.000000   \n",
       "mean       0.465671     32.895790      1.993623      3.759085      0.818833   \n",
       "std        0.498842     12.106833      0.640126      1.318487      0.385173   \n",
       "min        0.000000     15.000000      1.000000      1.000000      0.000000   \n",
       "25%        0.000000     24.000000      2.000000      3.000000      1.000000   \n",
       "50%        0.000000     30.000000      2.000000      4.000000      1.000000   \n",
       "75%        1.000000     40.000000      2.000000      5.000000      1.000000   \n",
       "max        1.000000     99.000000      5.000000      5.000000      1.000000   \n",
       "\n",
       "       urbanicity_f2f  account   account_fin  account_mob         saved  \\\n",
       "count    11448.000000  11448.0  11448.000000      11448.0  11448.000000   \n",
       "mean         0.478424      1.0      0.627446          1.0      0.787037   \n",
       "std          0.499556      0.0      0.483506          0.0      0.409420   \n",
       "min          0.000000      1.0      0.000000          1.0      0.000000   \n",
       "25%          0.000000      1.0      0.000000          1.0      1.000000   \n",
       "50%          0.000000      1.0      1.000000          1.0      1.000000   \n",
       "75%          1.000000      1.0      1.000000          1.0      1.000000   \n",
       "max          1.000000      1.0      1.000000          1.0      1.000000   \n",
       "\n",
       "           borrowed  receive_wages  receive_transfers  receive_pension  \\\n",
       "count  11448.000000   11448.000000       11448.000000     11448.000000   \n",
       "mean       0.710255       0.449773           0.149371         0.046209   \n",
       "std        0.453664       0.497493           0.356469         0.209946   \n",
       "min        0.000000       0.000000           0.000000         0.000000   \n",
       "25%        0.000000       0.000000           0.000000         0.000000   \n",
       "50%        1.000000       0.000000           0.000000         0.000000   \n",
       "75%        1.000000       1.000000           0.000000         0.000000   \n",
       "max        1.000000       1.000000           1.000000         1.000000   \n",
       "\n",
       "       receive_agriculture  pay_utilities   remittances   mobileowner  \\\n",
       "count         11448.000000   11448.000000  11448.000000  11448.000000   \n",
       "mean              0.196716       0.542628      0.962002      0.959644   \n",
       "std               0.397533       0.498201      0.191200      0.196802   \n",
       "min               0.000000       0.000000      0.000000      0.000000   \n",
       "25%               0.000000       0.000000      1.000000      1.000000   \n",
       "50%               0.000000       1.000000      1.000000      1.000000   \n",
       "75%               0.000000       1.000000      1.000000      1.000000   \n",
       "max               1.000000       1.000000      1.000000      1.000000   \n",
       "\n",
       "       internetaccess  anydigpayment  \n",
       "count    11448.000000        11448.0  \n",
       "mean         0.754630            1.0  \n",
       "std          0.430325            0.0  \n",
       "min          0.000000            1.0  \n",
       "25%          1.000000            1.0  \n",
       "50%          1.000000            1.0  \n",
       "75%          1.000000            1.0  \n",
       "max          1.000000            1.0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a note that the fin variables are Na to preserve sample size - we can try tossing them in to see if they increase predictive power later on\n",
    "df_selected[['female', 'age', 'educ', 'inc_q', 'emp_in', 'urbanicity_f2f', 'account', 'account_fin', 'account_mob', 'saved', 'borrowed', 'receive_wages', 'receive_transfers', 'receive_pension', 'receive_agriculture', 'pay_utilities', 'remittances', 'mobileowner', 'internetaccess', 'anydigpayment']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1fedf490-a923-4780-ac73-3b045511860a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchantpay_dig</th>\n",
       "      <th>receive_wages_scaled</th>\n",
       "      <th>receive_transfers_scaled</th>\n",
       "      <th>receive_pension_scaled</th>\n",
       "      <th>receive_agriculture_scaled</th>\n",
       "      <th>pay_utilities_scaled</th>\n",
       "      <th>remittances_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.477201</td>\n",
       "      <td>0.800052</td>\n",
       "      <td>0.281359</td>\n",
       "      <td>0.089535</td>\n",
       "      <td>0.294637</td>\n",
       "      <td>0.957547</td>\n",
       "      <td>1.828005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499502</td>\n",
       "      <td>0.927687</td>\n",
       "      <td>0.682798</td>\n",
       "      <td>0.410105</td>\n",
       "      <td>0.635376</td>\n",
       "      <td>0.933041</td>\n",
       "      <td>0.467362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       merchantpay_dig  receive_wages_scaled  receive_transfers_scaled  \\\n",
       "count     11448.000000          11448.000000              11448.000000   \n",
       "mean          0.477201              0.800052                  0.281359   \n",
       "std           0.499502              0.927687                  0.682798   \n",
       "min           0.000000              0.000000                  0.000000   \n",
       "25%           0.000000              0.000000                  0.000000   \n",
       "50%           0.000000              0.000000                  0.000000   \n",
       "75%           1.000000              2.000000                  0.000000   \n",
       "max           1.000000              2.000000                  2.000000   \n",
       "\n",
       "       receive_pension_scaled  receive_agriculture_scaled  \\\n",
       "count            11448.000000                11448.000000   \n",
       "mean                 0.089535                    0.294637   \n",
       "std                  0.410105                    0.635376   \n",
       "min                  0.000000                    0.000000   \n",
       "25%                  0.000000                    0.000000   \n",
       "50%                  0.000000                    0.000000   \n",
       "75%                  0.000000                    0.000000   \n",
       "max                  2.000000                    2.000000   \n",
       "\n",
       "       pay_utilities_scaled  remittances_scaled  \n",
       "count          11448.000000        11448.000000  \n",
       "mean               0.957547            1.828005  \n",
       "std                0.933041            0.467362  \n",
       "min                0.000000            0.000000  \n",
       "25%                0.000000            2.000000  \n",
       "50%                1.000000            2.000000  \n",
       "75%                2.000000            2.000000  \n",
       "max                2.000000            2.000000  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected[['merchantpay_dig','receive_wages_scaled', 'receive_transfers_scaled','receive_pension_scaled','receive_agriculture_scaled','pay_utilities_scaled', 'remittances_scaled' ]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "73cf3d03-5231-436a-a6c6-620d420f71fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fin2</th>\n",
       "      <th>fin4</th>\n",
       "      <th>fin5</th>\n",
       "      <th>fin6</th>\n",
       "      <th>fin7</th>\n",
       "      <th>fin8</th>\n",
       "      <th>fin9</th>\n",
       "      <th>fin10</th>\n",
       "      <th>fin13a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11426.000000</td>\n",
       "      <td>4940.000000</td>\n",
       "      <td>6575.000000</td>\n",
       "      <td>6577.000000</td>\n",
       "      <td>6570.000000</td>\n",
       "      <td>1684.000000</td>\n",
       "      <td>6566.000000</td>\n",
       "      <td>6561.000000</td>\n",
       "      <td>11448.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.432960</td>\n",
       "      <td>0.652024</td>\n",
       "      <td>0.777490</td>\n",
       "      <td>0.781359</td>\n",
       "      <td>0.256925</td>\n",
       "      <td>0.846793</td>\n",
       "      <td>0.820743</td>\n",
       "      <td>0.814358</td>\n",
       "      <td>0.785465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.495507</td>\n",
       "      <td>0.476376</td>\n",
       "      <td>0.415963</td>\n",
       "      <td>0.413356</td>\n",
       "      <td>0.436971</td>\n",
       "      <td>0.360294</td>\n",
       "      <td>0.383596</td>\n",
       "      <td>0.388847</td>\n",
       "      <td>0.410518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               fin2         fin4         fin5         fin6         fin7  \\\n",
       "count  11426.000000  4940.000000  6575.000000  6577.000000  6570.000000   \n",
       "mean       0.432960     0.652024     0.777490     0.781359     0.256925   \n",
       "std        0.495507     0.476376     0.415963     0.413356     0.436971   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        0.000000     0.000000     1.000000     1.000000     0.000000   \n",
       "50%        0.000000     1.000000     1.000000     1.000000     0.000000   \n",
       "75%        1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "max        1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "              fin8         fin9        fin10        fin13a  \n",
       "count  1684.000000  6566.000000  6561.000000  11448.000000  \n",
       "mean      0.846793     0.820743     0.814358      0.785465  \n",
       "std       0.360294     0.383596     0.388847      0.410518  \n",
       "min       0.000000     0.000000     0.000000      0.000000  \n",
       "25%       1.000000     1.000000     1.000000      1.000000  \n",
       "50%       1.000000     1.000000     1.000000      1.000000  \n",
       "75%       1.000000     1.000000     1.000000      1.000000  \n",
       "max       1.000000     1.000000     1.000000      1.000000  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected[['fin2', 'fin4', 'fin5', 'fin6', 'fin7', 'fin8', 'fin9', 'fin10', 'fin13a']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fcd07-59b1-4838-9fb3-6d6d1962acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# educ, economycode and income "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5553406-8879-4c21-bbce-ef2751d32113",
   "metadata": {},
   "source": [
    "# Comparing Regressions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1409e805-a0dc-40c7-abba-7b6b32c1200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_selected.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "24da32cf-150e-40a7-8eb5-f46551cadee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1486, 185)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5a516512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using only one-hot encoded variables first - no scales \n",
    "y=df_selected['fin13a']\n",
    "X=df_selected.drop(columns=['fin13a', 'economycode','merchantpay_dig','receive_wages_scaled', 'receive_transfers_scaled','receive_pension_scaled','receive_agriculture_scaled','pay_utilities_scaled', 'remittances_scaled', 'educ', 'inc_q' ])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcedc11-afd5-4b39-9007-9b6fb2640ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f096406b",
   "metadata": {},
   "source": [
    "#### 19. Run an OLS regression on the training data. Produce the summary statistics using .summary() and paste them here:\n",
    "\n",
    "Summary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eb964d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 fin13a   R-squared:                       0.149\n",
      "Model:                            OLS   Adj. R-squared:                  0.081\n",
      "Method:                 Least Squares   F-statistic:                     2.171\n",
      "Date:                Fri, 01 Mar 2024   Prob (F-statistic):           1.51e-09\n",
      "Time:                        12:47:06   Log-Likelihood:                -434.67\n",
      "No. Observations:                1337   AIC:                             1071.\n",
      "Df Residuals:                    1236   BIC:                             1596.\n",
      "Df Model:                         100                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "wgt                    -0.0074      0.019     -0.386      0.699      -0.045       0.030\n",
      "female                 -0.0076      0.021     -0.367      0.713      -0.048       0.033\n",
      "age                    -0.0012      0.001     -1.239      0.215      -0.003       0.001\n",
      "emp_in                 -0.0108      0.036     -0.302      0.762      -0.081       0.059\n",
      "urbanicity_f2f         -0.0378      0.033     -1.154      0.249      -0.102       0.026\n",
      "account                 0.1161      0.021      5.558      0.000       0.075       0.157\n",
      "account_fin             0.1161      0.021      5.558      0.000       0.075       0.157\n",
      "account_mob             0.1161      0.021      5.558      0.000       0.075       0.157\n",
      "fin2                    0.1161      0.021      5.558      0.000       0.075       0.157\n",
      "fin4                    0.0244      0.028      0.866      0.386      -0.031       0.080\n",
      "fin5                    0.0429      0.040      1.087      0.277      -0.035       0.120\n",
      "fin6                   -0.0403      0.042     -0.955      0.340      -0.123       0.042\n",
      "fin7                    0.1161      0.021      5.558      0.000       0.075       0.157\n",
      "fin8                   -0.0431      0.036     -1.183      0.237      -0.115       0.028\n",
      "fin9                   -0.0054      0.044     -0.123      0.902      -0.091       0.081\n",
      "fin10                   0.0583      0.041      1.435      0.152      -0.021       0.138\n",
      "saved                   0.0417      0.031      1.348      0.178      -0.019       0.102\n",
      "borrowed                0.0213      0.055      0.387      0.699      -0.087       0.129\n",
      "receive_wages           0.0070      0.023      0.300      0.765      -0.039       0.053\n",
      "receive_transfers      -0.0396      0.027     -1.493      0.136      -0.092       0.012\n",
      "receive_pension         0.0476      0.037      1.279      0.201      -0.025       0.121\n",
      "receive_agriculture    -0.0002      0.032     -0.006      0.995      -0.063       0.063\n",
      "pay_utilities           0.0337      0.026      1.319      0.187      -0.016       0.084\n",
      "remittances             0.0105      0.060      0.177      0.860      -0.106       0.127\n",
      "mobileowner             0.0131      0.086      0.152      0.879      -0.156       0.182\n",
      "internetaccess         -0.0029      0.047     -0.062      0.950      -0.094       0.089\n",
      "anydigpayment           0.1161      0.021      5.558      0.000       0.075       0.157\n",
      "educ_1                  0.0284      0.031      0.925      0.355      -0.032       0.089\n",
      "educ_2                  0.0458      0.018      2.490      0.013       0.010       0.082\n",
      "educ_3                  0.0419      0.021      1.988      0.047       0.001       0.083\n",
      "inc_q_1                -0.0067      0.040     -0.169      0.866      -0.084       0.071\n",
      "inc_q_2                 0.0052      0.031      0.169      0.866      -0.056       0.066\n",
      "inc_q_3                 0.0649      0.026      2.468      0.014       0.013       0.116\n",
      "inc_q_4                 0.0038      0.022      0.176      0.861      -0.039       0.047\n",
      "inc_q_5                 0.0489      0.020      2.430      0.015       0.009       0.088\n",
      "AFG                  4.989e-16   2.45e-16      2.038      0.042    1.87e-17    9.79e-16\n",
      "ALB                  3.951e-16    2.2e-16      1.798      0.072   -3.59e-17    8.26e-16\n",
      "ARE                  3.869e-16   2.57e-16      1.506      0.132   -1.17e-16    8.91e-16\n",
      "ARG                    -0.0482      0.047     -1.036      0.301      -0.140       0.043\n",
      "ARM                    -0.1676      0.109     -1.535      0.125      -0.382       0.047\n",
      "AUS                 -5.738e-17   2.08e-16     -0.276      0.783   -4.66e-16    3.51e-16\n",
      "AUT                  1.381e-16    1.5e-16      0.919      0.358   -1.57e-16    4.33e-16\n",
      "AZE                  3.113e-16   2.13e-16      1.460      0.145   -1.07e-16     7.3e-16\n",
      "BEL                  4.033e-17   1.98e-16      0.204      0.839   -3.48e-16    4.28e-16\n",
      "BEN                     0.1586      0.156      1.014      0.311      -0.148       0.465\n",
      "BFA                     0.1711      0.103      1.667      0.096      -0.030       0.372\n",
      "BGD                    -0.3434      0.248     -1.384      0.167      -0.830       0.144\n",
      "BGR                  -2.52e-16   3.25e-16     -0.775      0.439    -8.9e-16    3.86e-16\n",
      "BIH                 -9.203e-17   1.31e-16     -0.704      0.481   -3.48e-16    1.64e-16\n",
      "BOL                     0.0759      0.109      0.696      0.487      -0.138       0.290\n",
      "BRA                    -0.0606      0.051     -1.183      0.237      -0.161       0.040\n",
      "BWA                     0.1267      0.062      2.057      0.040       0.006       0.248\n",
      "CAN                  1.487e-16   1.31e-16      1.139      0.255   -1.07e-16    4.05e-16\n",
      "CHE                  2.472e-16      9e-17      2.746      0.006    7.06e-17    4.24e-16\n",
      "CHL                  4.513e-16   1.44e-16      3.141      0.002    1.69e-16    7.33e-16\n",
      "CHN                 -2.806e-16   1.71e-16     -1.643      0.101   -6.16e-16    5.45e-17\n",
      "CIV                     0.0548      0.107      0.513      0.608      -0.155       0.264\n",
      "CMR                     0.1858      0.125      1.486      0.138      -0.060       0.431\n",
      "COD                     0.1865      0.176      1.060      0.289      -0.159       0.532\n",
      "COG                     0.1964      0.175      1.120      0.263      -0.148       0.540\n",
      "COL                    -0.1473      0.065     -2.265      0.024      -0.275      -0.020\n",
      "COM                     0.1906      0.202      0.945      0.345      -0.205       0.587\n",
      "CRI                  1.581e-16   2.09e-16      0.755      0.451   -2.53e-16    5.69e-16\n",
      "CYP                  2.261e-16   2.42e-16      0.933      0.351   -2.49e-16    7.02e-16\n",
      "CZE                  4.674e-17   1.17e-16      0.399      0.690   -1.83e-16    2.76e-16\n",
      "DEU                  1.606e-16   1.04e-16      1.550      0.121   -4.27e-17    3.64e-16\n",
      "DNK                  2.691e-16   1.53e-16      1.758      0.079   -3.13e-17    5.69e-16\n",
      "DOM                    -0.4215      0.125     -3.371      0.001      -0.667      -0.176\n",
      "DZA                  1.043e-16   1.23e-16      0.849      0.396   -1.37e-16    3.45e-16\n",
      "ECU                 -1.876e-16   1.25e-16     -1.502      0.133   -4.33e-16    5.74e-17\n",
      "EGY                     0.1639      0.201      0.814      0.416      -0.231       0.559\n",
      "ESP                 -1.428e-16   1.39e-16     -1.026      0.305   -4.16e-16     1.3e-16\n",
      "EST                  2.001e-16   1.09e-16      1.842      0.066   -1.31e-17    4.13e-16\n",
      "ETH                  2.287e-16   9.84e-17      2.325      0.020    3.57e-17    4.22e-16\n",
      "FIN                 -3.232e-16   1.02e-16     -3.162      0.002   -5.24e-16   -1.23e-16\n",
      "FRA                 -3.652e-16   2.26e-16     -1.614      0.107   -8.09e-16    7.87e-17\n",
      "GAB                     0.1487      0.076      1.960      0.050      -0.000       0.298\n",
      "GBR                  4.106e-16   1.18e-16      3.474      0.001    1.79e-16    6.42e-16\n",
      "GEO                     0.1880      0.125      1.508      0.132      -0.057       0.433\n",
      "GHA                     0.1647      0.120      1.367      0.172      -0.072       0.401\n",
      "GIN                     0.1453      0.176      0.825      0.409      -0.200       0.491\n",
      "GMB                    -0.8096      0.246     -3.298      0.001      -1.291      -0.328\n",
      "GRC                  1.125e-17   1.06e-16      0.106      0.915   -1.97e-16    2.19e-16\n",
      "GTM                     0.1596      0.346      0.462      0.644      -0.519       0.838\n",
      "HKG                 -1.444e-16   8.61e-17     -1.678      0.094   -3.13e-16    2.44e-17\n",
      "HND                    -0.4514      0.157     -2.884      0.004      -0.759      -0.144\n",
      "HRV                  9.313e-18   5.79e-17      0.161      0.872   -1.04e-16    1.23e-16\n",
      "HUN                 -3.071e-17   5.48e-17     -0.561      0.575   -1.38e-16    7.68e-17\n",
      "IDN                    -0.0773      0.174     -0.443      0.658      -0.420       0.265\n",
      "IND                     0.1132      0.071      1.589      0.112      -0.027       0.253\n",
      "IRL                  3.101e-17   4.97e-17      0.623      0.533   -6.66e-17    1.29e-16\n",
      "IRN                 -4.345e-17   3.82e-17     -1.138      0.255   -1.18e-16    3.15e-17\n",
      "IRQ                     0.1738      0.202      0.861      0.390      -0.222       0.570\n",
      "ISL                  1.876e-16   1.01e-16      1.858      0.063   -1.05e-17    3.86e-16\n",
      "ISR                  1.035e-17   7.56e-17      0.137      0.891   -1.38e-16    1.59e-16\n",
      "ITA                 -2.415e-18   1.05e-16     -0.023      0.982   -2.09e-16    2.04e-16\n",
      "JAM                     0.0494      0.133      0.371      0.711      -0.212       0.310\n",
      "JOR                     0.1206      0.157      0.768      0.443      -0.188       0.429\n",
      "JPN                 -4.764e-17   2.67e-17     -1.783      0.075      -1e-16    4.79e-18\n",
      "KAZ                 -1.711e-17   1.81e-17     -0.944      0.345   -5.26e-17    1.84e-17\n",
      "KEN                     0.1235      0.077      1.596      0.111      -0.028       0.275\n",
      "KGZ                     0.1635      0.177      0.924      0.355      -0.184       0.511\n",
      "KHM                          0          0        nan        nan           0           0\n",
      "KOR                          0          0        nan        nan           0           0\n",
      "LAO                    -0.7835      0.350     -2.241      0.025      -1.469      -0.098\n",
      "LBN                          0          0        nan        nan           0           0\n",
      "LBR                     0.1970      0.124      1.587      0.113      -0.046       0.440\n",
      "LKA                    -0.2936      0.134     -2.193      0.029      -0.556      -0.031\n",
      "LSO                     0.1506      0.076      1.974      0.049       0.001       0.300\n",
      "LTU                          0          0        nan        nan           0           0\n",
      "LVA                          0          0        nan        nan           0           0\n",
      "MAR                     0.2617      0.354      0.739      0.460      -0.433       0.957\n",
      "MDA                          0          0        nan        nan           0           0\n",
      "MDG                     0.0435      0.101      0.431      0.666      -0.154       0.241\n",
      "MEX                     0.1221      0.084      1.457      0.145      -0.042       0.287\n",
      "MKD                          0          0        nan        nan           0           0\n",
      "MLI                    -0.0043      0.091     -0.047      0.962      -0.182       0.173\n",
      "MLT                          0          0        nan        nan           0           0\n",
      "MMR                     0.0540      0.098      0.553      0.580      -0.137       0.245\n",
      "MNG                    -0.0372      0.102     -0.365      0.715      -0.237       0.163\n",
      "MOZ                     0.0780      0.060      1.302      0.193      -0.040       0.196\n",
      "MRT                    -0.2784      0.248     -1.122      0.262      -0.765       0.208\n",
      "MUS                    -0.1744      0.080     -2.176      0.030      -0.332      -0.017\n",
      "MWI                     0.1842      0.245      0.750      0.453      -0.297       0.666\n",
      "MYS                     0.1264      0.089      1.416      0.157      -0.049       0.302\n",
      "NAM                     0.0900      0.044      2.068      0.039       0.005       0.175\n",
      "NER                     0.1628      0.202      0.807      0.420      -0.233       0.559\n",
      "NGA                     0.0185      0.125      0.148      0.882      -0.226       0.263\n",
      "NIC                    -0.0465      0.157     -0.297      0.767      -0.354       0.261\n",
      "NLD                          0          0        nan        nan           0           0\n",
      "NOR                          0          0        nan        nan           0           0\n",
      "NPL                    -0.1627      0.201     -0.810      0.418      -0.557       0.232\n",
      "NZL                          0          0        nan        nan           0           0\n",
      "PAK                    -0.1542      0.202     -0.764      0.445      -0.550       0.242\n",
      "PAN                          0          0        nan        nan           0           0\n",
      "PER                    -0.1059      0.107     -0.986      0.324      -0.317       0.105\n",
      "PHL                    -0.0383      0.058     -0.665      0.506      -0.151       0.075\n",
      "POL                          0          0        nan        nan           0           0\n",
      "PRT                          0          0        nan        nan           0           0\n",
      "PRY                     0.2115      0.203      1.043      0.297      -0.186       0.609\n",
      "PSE                     0.0915      0.347      0.264      0.792      -0.589       0.772\n",
      "ROU                          0          0        nan        nan           0           0\n",
      "RUS                    -0.3246      0.053     -6.069      0.000      -0.429      -0.220\n",
      "SAU                          0          0        nan        nan           0           0\n",
      "SEN                     0.0652      0.071      0.916      0.360      -0.074       0.205\n",
      "SGP                     0.0066      0.049      0.134      0.893      -0.090       0.103\n",
      "SLE                          0          0        nan        nan           0           0\n",
      "SLV                     0.2060      0.348      0.592      0.554      -0.477       0.889\n",
      "SRB                          0          0        nan        nan           0           0\n",
      "SSD                     0.1352      0.346      0.390      0.696      -0.544       0.814\n",
      "SVK                          0          0        nan        nan           0           0\n",
      "SVN                          0          0        nan        nan           0           0\n",
      "SWE                          0          0        nan        nan           0           0\n",
      "SWZ                     0.0085      0.066      0.129      0.897      -0.121       0.138\n",
      "TCD                     0.1295      0.090      1.443      0.149      -0.047       0.306\n",
      "TGO                     0.0634      0.119      0.532      0.595      -0.170       0.297\n",
      "THA                     0.0560      0.047      1.190      0.234      -0.036       0.148\n",
      "TJK                    -0.2139      0.249     -0.858      0.391      -0.703       0.275\n",
      "TUN                    -0.3361      0.246     -1.366      0.172      -0.819       0.147\n",
      "TUR                    -0.0499      0.064     -0.775      0.439      -0.176       0.076\n",
      "TWN                          0          0        nan        nan           0           0\n",
      "TZA                    -0.3506      0.246     -1.424      0.155      -0.834       0.132\n",
      "UGA                     0.1292      0.076      1.691      0.091      -0.021       0.279\n",
      "UKR                          0          0        nan        nan           0           0\n",
      "URY                          0          0        nan        nan           0           0\n",
      "USA                          0          0        nan        nan           0           0\n",
      "UZB                          0          0        nan        nan           0           0\n",
      "VEN                     0.0449      0.073      0.619      0.536      -0.097       0.187\n",
      "VNM                    -0.0523      0.104     -0.503      0.615      -0.257       0.152\n",
      "XKX                          0          0        nan        nan           0           0\n",
      "YEM                          0          0        nan        nan           0           0\n",
      "ZAF                     0.0744      0.059      1.264      0.206      -0.041       0.190\n",
      "ZMB                     0.0949      0.103      0.921      0.357      -0.107       0.297\n",
      "ZWE                     0.1832      0.158      1.160      0.246      -0.127       0.493\n",
      "==============================================================================\n",
      "Omnibus:                      350.428   Durbin-Watson:                   2.028\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              691.246\n",
      "Skew:                          -1.573   Prob(JB):                    7.90e-151\n",
      "Kurtosis:                       4.586   Cond. No.                     1.41e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 9.87e-27. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "# use weights for this! \n",
    "ols_result = sm.OLS(y_train, X_train).fit()\n",
    "print(ols_result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e805d",
   "metadata": {},
   "source": [
    "#### 20. Run a LASSO regression on the training data and search over different alpha parameters in using alphas=np.linspace(1e-6, 1, num=50). Hint: reference the review session example for coding LASSO.\n",
    "20a. What is the optimal alpha that you find?\n",
    "\n",
    "Answer: 0.020409\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e83a66cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040817285714285716\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "lassoReg = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
    "\n",
    "# Define parameter grid to search over using grid search\n",
    "alphas = np.linspace(1e-6, 1, num=50)\n",
    "params = {'lasso__alpha': alphas}\n",
    "\n",
    "# Set up the grid search\n",
    "gsLasso = GridSearchCV(lassoReg, params, n_jobs=-1, cv=10)\n",
    "\n",
    "# Fit gs to data\n",
    "gsLasso.fit(X, y)\n",
    "\n",
    "# Check best alpha\n",
    "print(gsLasso.best_params_['lasso__alpha'])\n",
    "best_lasso_alpha = gsLasso.best_params_['lasso__alpha']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0439645",
   "metadata": {},
   "source": [
    "20b. Insert a plot of alphas on the x axis and cv_errors on the y axis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aa14505e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the alpha values and corresponding mean test scores from the grid search results\n",
    "alphas = np.linspace(1e-6, 1, num=50)\n",
    "cv_errors = gsLasso.cv_results_['mean_test_score']\n",
    "\n",
    "plt.plot(alphas, cv_errors,'*')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1ef90",
   "metadata": {},
   "source": [
    "#### 21. Run a Ridge Regression on the training data and search over different alpha parameters in using alphas=np.linspace(1e-6, 1, num=50). Hint: reference the review session example for coding Ridge regressions.\n",
    "21a. What is the optimal alpha that you find? \n",
    "\n",
    "Answer here: 0.20408242857142858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3cb30753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Construct vector of alpha values\n",
    "alphas = np.linspace(1e-6, 1, num=50)\n",
    "\n",
    "# Construct vectors to store mean prediction errors and coefficients\n",
    "cv_errs = []\n",
    "coefs = []\n",
    "MSE = 10\n",
    "best_ridge_alpha = 0\n",
    "\n",
    "# Loop for running ridge regression for different values of alpha\n",
    "for a in alphas:\n",
    "    \n",
    "    # define pipeline object\n",
    "    ridgeReg = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha= a * X_train.shape[0]))\n",
    "    # run Ridge regression\n",
    "    ridgeReg.fit(X_train, y_train)\n",
    "    # obtain predicted values of output\n",
    "    y_pred = ridgeReg.predict(X_test)\n",
    "    # compute mean squared error\n",
    "    cv_errs.append(mean_squared_error(y_test, y_pred))\n",
    "    # store coefficients\n",
    "    coefs.append(ridgeReg['ridge'].coef_)\n",
    "    \n",
    "    # store value of alpha that minimizes the mean squared error\n",
    "    if mean_squared_error(y_test, y_pred) < MSE: #np.mean((y_pred - y_test)**2)\n",
    "        MSE = mean_squared_error(y_test, y_pred)\n",
    "        best_ridge_alpha = a\n",
    "        \n",
    "print(best_ridge_alpha)\n",
    "    \n",
    "# Create dataframe for storing coefficients\n",
    "coefs = pd.DataFrame(coefs, columns=X.columns)\n",
    "coefs.set_index(alphas, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4999f8a8",
   "metadata": {},
   "source": [
    "21b. Insert a plot of alphas on the x axis and cv_errros on the y axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9837dada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Mean Squared Error')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the MSE against the values of alphas\n",
    "plt.plot(alphas, cv_errs, '*')\n",
    "plt.xlabel(r'$\\alpha$')\n",
    "plt.ylabel('Mean Squared Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb94f7",
   "metadata": {},
   "source": [
    "#### 22. Compare the regression coefficients from the three different approaches using:\n",
    "``` python\n",
    "coef_comp=pd.DataFrame({'var':X.columns, 'val_ols':olsReg.params.tolist(), 'val_lasso':lassoReg.coef_, 'var_ridge':ridgeReg.coef_})\n",
    "```\n",
    "\n",
    "22a. Insert the coef_comp table here:\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "92d16f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;ridge&#x27;, Ridge(alpha=36.565010597564445))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;ridge&#x27;, Ridge(alpha=36.565010597564445))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_mean=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=36.565010597564445)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(with_mean=False)),\n",
       "                ('ridge', Ridge(alpha=36.565010597564445))])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = best_lasso_alpha\n",
    "lassoReg = make_pipeline(StandardScaler(with_mean=False), Lasso(alpha= alpha * np.sqrt(X_train.shape[0])))\n",
    "lassoReg.fit(X_train, y_train)\n",
    "\n",
    "alpha = best_ridge_alpha\n",
    "ridgeReg = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha= alpha * np.sqrt(X_train.shape[0])))\n",
    "ridgeReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7246c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_comp=pd.DataFrame({'var':X.columns, 'val_ols':ols_result.params.tolist(), 'val_lasso':lassoReg[-1].coef_, 'var_ridge':ridgeReg[-1].coef_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a67d6953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                var   val_ols  val_lasso  var_ridge\n",
      "0               wgt -0.007420       -0.0  -0.004609\n",
      "1            female -0.007554       -0.0  -0.003753\n",
      "2               age -0.001186       -0.0  -0.013536\n",
      "3            emp_in -0.010819       -0.0  -0.003244\n",
      "4    urbanicity_f2f -0.037760        0.0  -0.015090\n",
      "..              ...       ...        ...        ...\n",
      "169             XKX  0.000000        0.0   0.000000\n",
      "170             YEM  0.000000        0.0   0.000000\n",
      "171             ZAF  0.074380        0.0   0.010293\n",
      "172             ZMB  0.094894        0.0   0.007312\n",
      "173             ZWE  0.183224        0.0   0.010123\n",
      "\n",
      "[174 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(coef_comp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c3068",
   "metadata": {},
   "source": [
    "# V. CLASSIFICATION: LOGIT vs. NEURAL NETWORK\n",
    "\n",
    "#### 23. Load the titanic3.csv file in as \"df_titanic\" from HW2_data.zip in the Canvas Assignment for HW2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73975686",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic = pd.read_csv('titanic3.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8622d",
   "metadata": {},
   "source": [
    "#### 24. In Varian (2014), \"Big Data: New Tricks for Econometrics\", there is a discussion about wanting to allow for nonlinearity in age to affect the prediction of survival of Titanic passengers. This problem will compare estimating a logit with estimating neural network (multilayer perceptron). Explain the following lines of code and run them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db0d0624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71166718\n",
      "Iteration 2, loss = 0.69564375\n",
      "Iteration 3, loss = 0.68520857\n",
      "Iteration 4, loss = 0.67802560\n",
      "Iteration 5, loss = 0.67371194\n",
      "Iteration 6, loss = 0.67103982\n",
      "Iteration 7, loss = 0.66935104\n",
      "Iteration 8, loss = 0.66743090\n",
      "Iteration 9, loss = 0.66538807\n",
      "Iteration 10, loss = 0.66293164\n",
      "Iteration 11, loss = 0.66010487\n",
      "Iteration 12, loss = 0.65671367\n",
      "Iteration 13, loss = 0.65240347\n",
      "Iteration 14, loss = 0.64770667\n",
      "Iteration 15, loss = 0.64223625\n",
      "Iteration 16, loss = 0.63618211\n",
      "Iteration 17, loss = 0.62964568\n",
      "Iteration 18, loss = 0.62252273\n",
      "Iteration 19, loss = 0.61482248\n",
      "Iteration 20, loss = 0.60667495\n",
      "Iteration 21, loss = 0.59786557\n",
      "Iteration 22, loss = 0.58870687\n",
      "Iteration 23, loss = 0.57968533\n",
      "Iteration 24, loss = 0.57040901\n",
      "Iteration 25, loss = 0.56147817\n",
      "Iteration 26, loss = 0.55234323\n",
      "Iteration 27, loss = 0.54379160\n",
      "Iteration 28, loss = 0.53601911\n",
      "Iteration 29, loss = 0.52869627\n",
      "Iteration 30, loss = 0.52233033\n",
      "Iteration 31, loss = 0.51599733\n",
      "Iteration 32, loss = 0.51011936\n",
      "Iteration 33, loss = 0.50522173\n",
      "Iteration 34, loss = 0.50075591\n",
      "Iteration 35, loss = 0.49664485\n",
      "Iteration 36, loss = 0.49283276\n",
      "Iteration 37, loss = 0.48952047\n",
      "Iteration 38, loss = 0.48656204\n",
      "Iteration 39, loss = 0.48355458\n",
      "Iteration 40, loss = 0.48150140\n",
      "Iteration 41, loss = 0.47919972\n",
      "Iteration 42, loss = 0.47725189\n",
      "Iteration 43, loss = 0.47533007\n",
      "Iteration 44, loss = 0.47361908\n",
      "Iteration 45, loss = 0.47221271\n",
      "Iteration 46, loss = 0.47108516\n",
      "Iteration 47, loss = 0.46975546\n",
      "Iteration 48, loss = 0.46866720\n",
      "Iteration 49, loss = 0.46774065\n",
      "Iteration 50, loss = 0.46696209\n",
      "Iteration 51, loss = 0.46543506\n",
      "Iteration 52, loss = 0.46442761\n",
      "Iteration 53, loss = 0.46327225\n",
      "Iteration 54, loss = 0.46259162\n",
      "Iteration 55, loss = 0.46239550\n",
      "Iteration 56, loss = 0.46108839\n",
      "Iteration 57, loss = 0.46041167\n",
      "Iteration 58, loss = 0.45981930\n",
      "Iteration 59, loss = 0.45935899\n",
      "Iteration 60, loss = 0.45879618\n",
      "Iteration 61, loss = 0.45814538\n",
      "Iteration 62, loss = 0.45757360\n",
      "Iteration 63, loss = 0.45668898\n",
      "Iteration 64, loss = 0.45643084\n",
      "Iteration 65, loss = 0.45604896\n",
      "Iteration 66, loss = 0.45555667\n",
      "Iteration 67, loss = 0.45498798\n",
      "Iteration 68, loss = 0.45465742\n",
      "Iteration 69, loss = 0.45389672\n",
      "Iteration 70, loss = 0.45400424\n",
      "Iteration 71, loss = 0.45339990\n",
      "Iteration 72, loss = 0.45294198\n",
      "Iteration 73, loss = 0.45258002\n",
      "Iteration 74, loss = 0.45225680\n",
      "Iteration 75, loss = 0.45146065\n",
      "Iteration 76, loss = 0.45077920\n",
      "Iteration 77, loss = 0.45073901\n",
      "Iteration 78, loss = 0.45035627\n",
      "Iteration 79, loss = 0.45016523\n",
      "Iteration 80, loss = 0.44973614\n",
      "Iteration 81, loss = 0.44932752\n",
      "Iteration 82, loss = 0.44890936\n",
      "Iteration 83, loss = 0.44868632\n",
      "Iteration 84, loss = 0.44825281\n",
      "Iteration 85, loss = 0.44782804\n",
      "Iteration 86, loss = 0.44750819\n",
      "Iteration 87, loss = 0.44807662\n",
      "Iteration 88, loss = 0.44817499\n",
      "Iteration 89, loss = 0.44776845\n",
      "Iteration 90, loss = 0.44689111\n",
      "Iteration 91, loss = 0.44629861\n",
      "Iteration 92, loss = 0.44605975\n",
      "Iteration 93, loss = 0.44598846\n",
      "Iteration 94, loss = 0.44601327\n",
      "Iteration 95, loss = 0.44608612\n",
      "Iteration 96, loss = 0.44580171\n",
      "Iteration 97, loss = 0.44520288\n",
      "Iteration 98, loss = 0.44467889\n",
      "Iteration 99, loss = 0.44483546\n",
      "Iteration 100, loss = 0.44494537\n",
      "Iteration 101, loss = 0.44478049\n",
      "Iteration 102, loss = 0.44429273\n",
      "Iteration 103, loss = 0.44415379\n",
      "Iteration 104, loss = 0.44353009\n",
      "Iteration 105, loss = 0.44343126\n",
      "Iteration 106, loss = 0.44336633\n",
      "Iteration 107, loss = 0.44375810\n",
      "Iteration 108, loss = 0.44308108\n",
      "Iteration 109, loss = 0.44302754\n",
      "Iteration 110, loss = 0.44269672\n",
      "Iteration 111, loss = 0.44274081\n",
      "Iteration 112, loss = 0.44293671\n",
      "Iteration 113, loss = 0.44265968\n",
      "Iteration 114, loss = 0.44245009\n",
      "Iteration 115, loss = 0.44237114\n",
      "Iteration 116, loss = 0.44206141\n",
      "Iteration 117, loss = 0.44210746\n",
      "Iteration 118, loss = 0.44212929\n",
      "Iteration 119, loss = 0.44173635\n",
      "Iteration 120, loss = 0.44172917\n",
      "Iteration 121, loss = 0.44150225\n",
      "Iteration 122, loss = 0.44146846\n",
      "Iteration 123, loss = 0.44137832\n",
      "Iteration 124, loss = 0.44132983\n",
      "Iteration 125, loss = 0.44114690\n",
      "Iteration 126, loss = 0.44100544\n",
      "Iteration 127, loss = 0.44188374\n",
      "Iteration 128, loss = 0.44116454\n",
      "Iteration 129, loss = 0.44098902\n",
      "Iteration 130, loss = 0.44116801\n",
      "Iteration 131, loss = 0.44085864\n",
      "Iteration 132, loss = 0.44091981\n",
      "Iteration 133, loss = 0.44049528\n",
      "Iteration 134, loss = 0.44053138\n",
      "Iteration 135, loss = 0.44053738\n",
      "Iteration 136, loss = 0.44033819\n",
      "Iteration 137, loss = 0.44024998\n",
      "Iteration 138, loss = 0.44016534\n",
      "Iteration 139, loss = 0.44010142\n",
      "Iteration 140, loss = 0.44028315\n",
      "Iteration 141, loss = 0.44021719\n",
      "Iteration 142, loss = 0.43998023\n",
      "Iteration 143, loss = 0.43974332\n",
      "Iteration 144, loss = 0.43971696\n",
      "Iteration 145, loss = 0.43971643\n",
      "Iteration 146, loss = 0.43953876\n",
      "Iteration 147, loss = 0.43957465\n",
      "Iteration 148, loss = 0.43982144\n",
      "Iteration 149, loss = 0.44006412\n",
      "Iteration 150, loss = 0.43938363\n",
      "Iteration 151, loss = 0.43938990\n",
      "Iteration 152, loss = 0.43941845\n",
      "Iteration 153, loss = 0.43934152\n",
      "Iteration 154, loss = 0.43950925\n",
      "Iteration 155, loss = 0.43920843\n",
      "Iteration 156, loss = 0.43904215\n",
      "Iteration 157, loss = 0.43907895\n",
      "Iteration 158, loss = 0.43889633\n",
      "Iteration 159, loss = 0.43877681\n",
      "Iteration 160, loss = 0.43871036\n",
      "Iteration 161, loss = 0.43871964\n",
      "Iteration 162, loss = 0.43856255\n",
      "Iteration 163, loss = 0.43866152\n",
      "Iteration 164, loss = 0.43863636\n",
      "Iteration 165, loss = 0.43858604\n",
      "Iteration 166, loss = 0.43849784\n",
      "Iteration 167, loss = 0.43868626\n",
      "Iteration 168, loss = 0.43871881\n",
      "Iteration 169, loss = 0.43838606\n",
      "Iteration 170, loss = 0.43830630\n",
      "Iteration 171, loss = 0.43832139\n",
      "Iteration 172, loss = 0.43824694\n",
      "Iteration 173, loss = 0.43819155\n",
      "Iteration 174, loss = 0.43819710\n",
      "Iteration 175, loss = 0.43904735\n",
      "Iteration 176, loss = 0.43926521\n",
      "Iteration 177, loss = 0.43832611\n",
      "Iteration 178, loss = 0.43816531\n",
      "Iteration 179, loss = 0.43798871\n",
      "Iteration 180, loss = 0.43816875\n",
      "Iteration 181, loss = 0.43813913\n",
      "Iteration 182, loss = 0.43819705\n",
      "Iteration 183, loss = 0.43782033\n",
      "Iteration 184, loss = 0.43774545\n",
      "Iteration 185, loss = 0.43780127\n",
      "Iteration 186, loss = 0.43764685\n",
      "Iteration 187, loss = 0.43770271\n",
      "Iteration 188, loss = 0.43768801\n",
      "Iteration 189, loss = 0.43747252\n",
      "Iteration 190, loss = 0.43778345\n",
      "Iteration 191, loss = 0.43776920\n",
      "Iteration 192, loss = 0.43783604\n",
      "Iteration 193, loss = 0.43757698\n",
      "Iteration 194, loss = 0.43753950\n",
      "Iteration 195, loss = 0.43766676\n",
      "Iteration 196, loss = 0.43742867\n",
      "Iteration 197, loss = 0.43769370\n",
      "Iteration 198, loss = 0.43698000\n",
      "Iteration 199, loss = 0.43755444\n",
      "Iteration 200, loss = 0.43886079\n",
      "Iteration 201, loss = 0.43857381\n",
      "Iteration 202, loss = 0.43775442\n",
      "Iteration 203, loss = 0.43736451\n",
      "Iteration 204, loss = 0.43713683\n",
      "Iteration 205, loss = 0.43761689\n",
      "Iteration 206, loss = 0.43695774\n",
      "Iteration 207, loss = 0.43717902\n",
      "Iteration 208, loss = 0.43735451\n",
      "Iteration 209, loss = 0.43767873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP Training Accuracy: 0.8038277511961722\n",
      "MLP Testing Accuracy: 0.7952380952380952\n",
      "Logit Training Accuracy: 0.7882775119617225\n",
      "Logit Testing Accuracy: 0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "df_titanic.dropna(subset=['survived','age', 'sex','pclass'],inplace=True) \n",
    "# Create a female dummy variable \n",
    "df_titanic['female']= np.where(df_titanic['sex']=='female',1,0) \n",
    "# Assign input and output variables, split data into train and test \n",
    "y= df_titanic['survived']\n",
    "X= df_titanic[['age','pclass', 'female']] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1680) \n",
    "\n",
    "# Standardize the features\n",
    "scaler=StandardScaler() \n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# train the multilayer perceptron classifier \n",
    "MLP = MLPClassifier(hidden_layer_sizes=(4,2),\n",
    "  random_state=1680,\n",
    "                    activation='logistic', solver='adam', \n",
    "                    max_iter =500,\n",
    "                    verbose=True, learning_rate_init=0.01) \n",
    "MLP.fit(X_train,y_train)\n",
    "print(\"MLP Training Accuracy:\",accuracy_score(y_train,MLP.predict(X_train)))\n",
    "print(\"MLP Testing Accuracy:\",accuracy_score(y_test, MLP.predict(X_test)))\n",
    "\n",
    "# train the logistic regression model \n",
    "logitmodel = LogisticRegression(solver='liblinear', random_state=1680).fit(X_train, y_train) \n",
    "print(\"Logit Training Accuracy:\", accuracy_score(y_train,logitmodel.predict(X_train)))\n",
    "print(\"Logit Testing Accuracy:\", accuracy_score(y_test, logitmodel.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9641e347",
   "metadata": {},
   "source": [
    "24a. Compare the accuracy scores in-sample and out-of-sample for the logit regression and for the neural network in 2-3 sentences. \n",
    "\n",
    "Answer here: The neural network performed better both in-sample (training accuracy) and out of sample (testing accuracy). However, the differences were not extremely noticable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae2e97e",
   "metadata": {},
   "source": [
    "24b. Is the difference between the accuracies what you would have expected? Spend 2-3 sentences discussing possible explanations for the differences. \n",
    "\n",
    "Answer here: These differences in accuracies are expected, since a linear model like logistic regression is likely unable to discover more nuances in the data, as the paper explains. However, it is interesting that the gap in accuracy scores is not as big as expected, showing that even if logistic regression may not uncover all hidden trends, it is still a relatively powerful predictor. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7101d",
   "metadata": {},
   "source": [
    "24c. Print out the descriptive statistics for y_train and y_test\n",
    "\n",
    "Paste output here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896ff243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for y_train:\n",
      "count    836.000000\n",
      "mean       0.399522\n",
      "std        0.490093\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: survived, dtype: float64\n",
      "\n",
      "Descriptive Statistics for y_test:\n",
      "count    210.000000\n",
      "mean       0.442857\n",
      "std        0.497911\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Descriptive Statistics for y_train:\")\n",
    "print(y_train.describe())\n",
    "\n",
    "print(\"\\nDescriptive Statistics for y_test:\")\n",
    "print(y_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86368c1e",
   "metadata": {},
   "source": [
    "#### 25. Now rerun the previous code, but add stratify=y into the train_test_split command.\n",
    "\n",
    "25a. How do the descriptive statistics for y_train and y_test change and how do the accuracy scores change?\n",
    "\n",
    "Answer here: The accuracy score for logistic regression decreased for the test dataset, whereas the MLP network accuracy remained fairly constant. The mean and std for y_train and y_test is now more evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291119ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70894753\n",
      "Iteration 2, loss = 0.69741544\n",
      "Iteration 3, loss = 0.68669943\n",
      "Iteration 4, loss = 0.68101162\n",
      "Iteration 5, loss = 0.67693919\n",
      "Iteration 6, loss = 0.67398654\n",
      "Iteration 7, loss = 0.67215091\n",
      "Iteration 8, loss = 0.67096901\n",
      "Iteration 9, loss = 0.66898181\n",
      "Iteration 10, loss = 0.66659063\n",
      "Iteration 11, loss = 0.66320451\n",
      "Iteration 12, loss = 0.65927254\n",
      "Iteration 13, loss = 0.65449687\n",
      "Iteration 14, loss = 0.64898397\n",
      "Iteration 15, loss = 0.64192832\n",
      "Iteration 16, loss = 0.63462357\n",
      "Iteration 17, loss = 0.62531451\n",
      "Iteration 18, loss = 0.61571887\n",
      "Iteration 19, loss = 0.60527922\n",
      "Iteration 20, loss = 0.59445752\n",
      "Iteration 21, loss = 0.58344477\n",
      "Iteration 22, loss = 0.57183734\n",
      "Iteration 23, loss = 0.56036779\n",
      "Iteration 24, loss = 0.54944270\n",
      "Iteration 25, loss = 0.53896643\n",
      "Iteration 26, loss = 0.52987271\n",
      "Iteration 27, loss = 0.52153519\n",
      "Iteration 28, loss = 0.51403032\n",
      "Iteration 29, loss = 0.50749977\n",
      "Iteration 30, loss = 0.50196190\n",
      "Iteration 31, loss = 0.49711786\n",
      "Iteration 32, loss = 0.49308008\n",
      "Iteration 33, loss = 0.48970534\n",
      "Iteration 34, loss = 0.48651464\n",
      "Iteration 35, loss = 0.48344136\n",
      "Iteration 36, loss = 0.48084612\n",
      "Iteration 37, loss = 0.47854965\n",
      "Iteration 38, loss = 0.47662251\n",
      "Iteration 39, loss = 0.47484221\n",
      "Iteration 40, loss = 0.47290417\n",
      "Iteration 41, loss = 0.47148746\n",
      "Iteration 42, loss = 0.47022571\n",
      "Iteration 43, loss = 0.46899535\n",
      "Iteration 44, loss = 0.46797152\n",
      "Iteration 45, loss = 0.46716896\n",
      "Iteration 46, loss = 0.46607139\n",
      "Iteration 47, loss = 0.46496629\n",
      "Iteration 48, loss = 0.46448952\n",
      "Iteration 49, loss = 0.46318059\n",
      "Iteration 50, loss = 0.46232900\n",
      "Iteration 51, loss = 0.46160886\n",
      "Iteration 52, loss = 0.46081428\n",
      "Iteration 53, loss = 0.46001537\n",
      "Iteration 54, loss = 0.45954855\n",
      "Iteration 55, loss = 0.45907291\n",
      "Iteration 56, loss = 0.45810280\n",
      "Iteration 57, loss = 0.45773429\n",
      "Iteration 58, loss = 0.45735887\n",
      "Iteration 59, loss = 0.45662992\n",
      "Iteration 60, loss = 0.45569166\n",
      "Iteration 61, loss = 0.45501053\n",
      "Iteration 62, loss = 0.45459642\n",
      "Iteration 63, loss = 0.45432366\n",
      "Iteration 64, loss = 0.45357091\n",
      "Iteration 65, loss = 0.45319200\n",
      "Iteration 66, loss = 0.45247689\n",
      "Iteration 67, loss = 0.45292822\n",
      "Iteration 68, loss = 0.45337949\n",
      "Iteration 69, loss = 0.45280203\n",
      "Iteration 70, loss = 0.45199738\n",
      "Iteration 71, loss = 0.45073036\n",
      "Iteration 72, loss = 0.45009326\n",
      "Iteration 73, loss = 0.44958757\n",
      "Iteration 74, loss = 0.44964337\n",
      "Iteration 75, loss = 0.44940204\n",
      "Iteration 76, loss = 0.44922017\n",
      "Iteration 77, loss = 0.44951484\n",
      "Iteration 78, loss = 0.44864363\n",
      "Iteration 79, loss = 0.44801162\n",
      "Iteration 80, loss = 0.44782928\n",
      "Iteration 81, loss = 0.44754956\n",
      "Iteration 82, loss = 0.44737481\n",
      "Iteration 83, loss = 0.44720824\n",
      "Iteration 84, loss = 0.44651266\n",
      "Iteration 85, loss = 0.44617380\n",
      "Iteration 86, loss = 0.44599485\n",
      "Iteration 87, loss = 0.44566158\n",
      "Iteration 88, loss = 0.44544934\n",
      "Iteration 89, loss = 0.44525506\n",
      "Iteration 90, loss = 0.44503439\n",
      "Iteration 91, loss = 0.44460125\n",
      "Iteration 92, loss = 0.44442496\n",
      "Iteration 93, loss = 0.44427671\n",
      "Iteration 94, loss = 0.44410551\n",
      "Iteration 95, loss = 0.44448040\n",
      "Iteration 96, loss = 0.44416481\n",
      "Iteration 97, loss = 0.44382579\n",
      "Iteration 98, loss = 0.44352747\n",
      "Iteration 99, loss = 0.44332383\n",
      "Iteration 100, loss = 0.44311279\n",
      "Iteration 101, loss = 0.44302381\n",
      "Iteration 102, loss = 0.44299839\n",
      "Iteration 103, loss = 0.44350262\n",
      "Iteration 104, loss = 0.44320671\n",
      "Iteration 105, loss = 0.44324638\n",
      "Iteration 106, loss = 0.44170535\n",
      "Iteration 107, loss = 0.44165513\n",
      "Iteration 108, loss = 0.44167488\n",
      "Iteration 109, loss = 0.44133792\n",
      "Iteration 110, loss = 0.44116415\n",
      "Iteration 111, loss = 0.44159924\n",
      "Iteration 112, loss = 0.44169044\n",
      "Iteration 113, loss = 0.44122010\n",
      "Iteration 114, loss = 0.44054329\n",
      "Iteration 115, loss = 0.44028948\n",
      "Iteration 116, loss = 0.43990991\n",
      "Iteration 117, loss = 0.44016751\n",
      "Iteration 118, loss = 0.43978895\n",
      "Iteration 119, loss = 0.43923108\n",
      "Iteration 120, loss = 0.43943849\n",
      "Iteration 121, loss = 0.43963478\n",
      "Iteration 122, loss = 0.43990661\n",
      "Iteration 123, loss = 0.43948293\n",
      "Iteration 124, loss = 0.43876472\n",
      "Iteration 125, loss = 0.43849687\n",
      "Iteration 126, loss = 0.43845536\n",
      "Iteration 127, loss = 0.43833323\n",
      "Iteration 128, loss = 0.43883088\n",
      "Iteration 129, loss = 0.43808958\n",
      "Iteration 130, loss = 0.43754464\n",
      "Iteration 131, loss = 0.43706371\n",
      "Iteration 132, loss = 0.43681648\n",
      "Iteration 133, loss = 0.43687862\n",
      "Iteration 134, loss = 0.43667614\n",
      "Iteration 135, loss = 0.43649638\n",
      "Iteration 136, loss = 0.43607459\n",
      "Iteration 137, loss = 0.43577466\n",
      "Iteration 138, loss = 0.43552242\n",
      "Iteration 139, loss = 0.43521842\n",
      "Iteration 140, loss = 0.43572647\n",
      "Iteration 141, loss = 0.43583102\n",
      "Iteration 142, loss = 0.43576446\n",
      "Iteration 143, loss = 0.43480281\n",
      "Iteration 144, loss = 0.43481371\n",
      "Iteration 145, loss = 0.43437316\n",
      "Iteration 146, loss = 0.43427433\n",
      "Iteration 147, loss = 0.43436273\n",
      "Iteration 148, loss = 0.43456451\n",
      "Iteration 149, loss = 0.43388004\n",
      "Iteration 150, loss = 0.43296748\n",
      "Iteration 151, loss = 0.43312479\n",
      "Iteration 152, loss = 0.43414538\n",
      "Iteration 153, loss = 0.43433527\n",
      "Iteration 154, loss = 0.43382614\n",
      "Iteration 155, loss = 0.43285482\n",
      "Iteration 156, loss = 0.43240875\n",
      "Iteration 157, loss = 0.43259600\n",
      "Iteration 158, loss = 0.43230690\n",
      "Iteration 159, loss = 0.43193629\n",
      "Iteration 160, loss = 0.43204670\n",
      "Iteration 161, loss = 0.43181159\n",
      "Iteration 162, loss = 0.43197666\n",
      "Iteration 163, loss = 0.43148609\n",
      "Iteration 164, loss = 0.43083452\n",
      "Iteration 165, loss = 0.43070318\n",
      "Iteration 166, loss = 0.43060605\n",
      "Iteration 167, loss = 0.43052166\n",
      "Iteration 168, loss = 0.43036037\n",
      "Iteration 169, loss = 0.43032264\n",
      "Iteration 170, loss = 0.43021488\n",
      "Iteration 171, loss = 0.43004933\n",
      "Iteration 172, loss = 0.43007661\n",
      "Iteration 173, loss = 0.43014175\n",
      "Iteration 174, loss = 0.43004125\n",
      "Iteration 175, loss = 0.42982602\n",
      "Iteration 176, loss = 0.42983218\n",
      "Iteration 177, loss = 0.42954329\n",
      "Iteration 178, loss = 0.42920220\n",
      "Iteration 179, loss = 0.42896831\n",
      "Iteration 180, loss = 0.42871912\n",
      "Iteration 181, loss = 0.42875433\n",
      "Iteration 182, loss = 0.42866140\n",
      "Iteration 183, loss = 0.42917229\n",
      "Iteration 184, loss = 0.42934972\n",
      "Iteration 185, loss = 0.42953907\n",
      "Iteration 186, loss = 0.42974757\n",
      "Iteration 187, loss = 0.43042244\n",
      "Iteration 188, loss = 0.42969124\n",
      "Iteration 189, loss = 0.42875240\n",
      "Iteration 190, loss = 0.42800716\n",
      "Iteration 191, loss = 0.42759129\n",
      "Iteration 192, loss = 0.42786322\n",
      "Iteration 193, loss = 0.42742346\n",
      "Iteration 194, loss = 0.42772104\n",
      "Iteration 195, loss = 0.42745466\n",
      "Iteration 196, loss = 0.42724313\n",
      "Iteration 197, loss = 0.42718933\n",
      "Iteration 198, loss = 0.42700708\n",
      "Iteration 199, loss = 0.42731125\n",
      "Iteration 200, loss = 0.42717705\n",
      "Iteration 201, loss = 0.42717082\n",
      "Iteration 202, loss = 0.42695755\n",
      "Iteration 203, loss = 0.42683940\n",
      "Iteration 204, loss = 0.42669106\n",
      "Iteration 205, loss = 0.42688594\n",
      "Iteration 206, loss = 0.42673341\n",
      "Iteration 207, loss = 0.42669542\n",
      "Iteration 208, loss = 0.42693887\n",
      "Iteration 209, loss = 0.42664727\n",
      "Iteration 210, loss = 0.42649493\n",
      "Iteration 211, loss = 0.42621209\n",
      "Iteration 212, loss = 0.42750075\n",
      "Iteration 213, loss = 0.42816448\n",
      "Iteration 214, loss = 0.42686658\n",
      "Iteration 215, loss = 0.42604203\n",
      "Iteration 216, loss = 0.42676958\n",
      "Iteration 217, loss = 0.42670164\n",
      "Iteration 218, loss = 0.42655304\n",
      "Iteration 219, loss = 0.42612958\n",
      "Iteration 220, loss = 0.42638685\n",
      "Iteration 221, loss = 0.42622857\n",
      "Iteration 222, loss = 0.42595935\n",
      "Iteration 223, loss = 0.42574123\n",
      "Iteration 224, loss = 0.42575017\n",
      "Iteration 225, loss = 0.42581423\n",
      "Iteration 226, loss = 0.42583776\n",
      "Iteration 227, loss = 0.42600166\n",
      "Iteration 228, loss = 0.42580239\n",
      "Iteration 229, loss = 0.42569681\n",
      "Iteration 230, loss = 0.42568530\n",
      "Iteration 231, loss = 0.42556476\n",
      "Iteration 232, loss = 0.42546466\n",
      "Iteration 233, loss = 0.42542519\n",
      "Iteration 234, loss = 0.42582979\n",
      "Iteration 235, loss = 0.42631157\n",
      "Iteration 236, loss = 0.42625451\n",
      "Iteration 237, loss = 0.42558842\n",
      "Iteration 238, loss = 0.42542647\n",
      "Iteration 239, loss = 0.42559623\n",
      "Iteration 240, loss = 0.42551892\n",
      "Iteration 241, loss = 0.42536390\n",
      "Iteration 242, loss = 0.42534441\n",
      "Iteration 243, loss = 0.42534549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP Training Accuracy: 0.8026315789473685\n",
      "MLP Testing Accuracy: 0.7952380952380952\n",
      "Logit Training Accuracy: 0.7954545454545454\n",
      "Logit Testing Accuracy: 0.7571428571428571\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1680, stratify=y) \n",
    "\n",
    "# Standardize the features\n",
    "scaler=StandardScaler() \n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# train the multilayer perceptron classifier \n",
    "MLP = MLPClassifier(hidden_layer_sizes=(4,2),\n",
    "  random_state=1680,\n",
    "                    activation='logistic', solver='adam', \n",
    "                    max_iter =500,\n",
    "                    verbose=True, learning_rate_init=0.01) \n",
    "MLP.fit(X_train,y_train)\n",
    "print(\"MLP Training Accuracy:\",accuracy_score(y_train,MLP.predict(X_train)))\n",
    "print(\"MLP Testing Accuracy:\",accuracy_score(y_test, MLP.predict(X_test)))\n",
    "\n",
    "# train the logistic regression model \n",
    "logitmodel = LogisticRegression(solver='liblinear', random_state=1680).fit(X_train, y_train) \n",
    "print(\"Logit Training Accuracy:\", accuracy_score(y_train,logitmodel.predict(X_train)))\n",
    "print(\"Logit Testing Accuracy:\", accuracy_score(y_test, logitmodel.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764d9d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for y_train:\n",
      "count    836.000000\n",
      "mean       0.407895\n",
      "std        0.491738\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: survived, dtype: float64\n",
      "\n",
      "Descriptive Statistics for y_test:\n",
      "count    210.000000\n",
      "mean       0.409524\n",
      "std        0.492921\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Descriptive Statistics for y_train:\")\n",
    "print(y_train.describe())\n",
    "\n",
    "print(\"\\nDescriptive Statistics for y_test:\")\n",
    "print(y_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f274b",
   "metadata": {},
   "source": [
    "25b. What does “stratify” do and why would it change your results?\n",
    "\n",
    "Answer here:  Stratify makes the y_train and y_test datasets more similar in distribution in terms of y, which ensures that the model is trained and evaluated on representative samples of each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d0460",
   "metadata": {},
   "source": [
    "#### 26. Look at the other variables in df_titanic. What other variables besides age do you think would be important in predicting survival?\n",
    "\n",
    "26a. List the variables you think of as important here and explain why you think they would improve prediction. Hint: you may need to transform variables into numerical representations/dummy variables.\n",
    "\n",
    "Answer here: In addition to age, sex and class, I chose parch (number of parents and children) since I suspect having family on board may slow down evacuation. Furthermore, I chose embarked to capture geographical and cultural differences between individuals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6346faf",
   "metadata": {},
   "source": [
    "26b. Run the logit regression with the variables that you listed and print the accuracy below. Did your accuracy improve?\n",
    "\n",
    "Answer here: We can see that the training accuracy decreased slightly, but the testing accuarcy increased slightly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9e80d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit Training Accuracy: 0.7892215568862275\n",
      "Logit Testing Accuracy: 0.7799043062200957\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "df_titanic.dropna(subset=['survived','age', 'sex','pclass', 'parch', 'embarked'],inplace=True) \n",
    "# Create a female dummy variable \n",
    "df_titanic['female']= np.where(df_titanic['sex']=='female',1,0) \n",
    "# Create a has family dummy variable \n",
    "df_titanic['family']= np.where(df_titanic['parch']==0,0,1)\n",
    "# Create dummies for each port of embarkation \n",
    "df_titanic['Cherbourg']= np.where(df_titanic['embarked']=='C',1,0)\n",
    "df_titanic['Queenstown']= np.where(df_titanic['embarked']=='Q',1,0)\n",
    "df_titanic['Southampton']= np.where(df_titanic['embarked']=='S',1,0)\n",
    "\n",
    "# Assign input and output variables, split data into train and test \n",
    "y= df_titanic['survived']\n",
    "X= df_titanic[['age','pclass', 'female', 'family', 'Cherbourg', 'Queenstown', 'Southampton']] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1680, stratify=y) \n",
    "\n",
    "# Standardize the features\n",
    "scaler=StandardScaler() \n",
    "scaler.fit(X_train) \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# train the logistic regression model \n",
    "logitmodel = LogisticRegression(solver='liblinear', random_state=1680).fit(X_train, y_train) \n",
    "print(\"Logit Training Accuracy:\", accuracy_score(y_train,logitmodel.predict(X_train)))\n",
    "print(\"Logit Testing Accuracy:\", accuracy_score(y_test, logitmodel.predict(X_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff6ebcde2956354653a1bfc837161093b4abe74b5e977ba193ed76204ade37d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
