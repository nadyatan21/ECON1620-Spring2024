{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bae82a9-d828-4778-866f-bb33846c12db",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "## Econ 1680: MLTA and Econ\n",
    "\n",
    "#### Name: Nadya Tan\n",
    "\n",
    "In this course we will be using Python for writing code to apply machine learning and text analysis methods to economics topics. Python is free, flexible, offers a variety of predefined packages, and is popular. It can handle everything from the statistical analysis of Stata to the matrix algebra and simulation of Matlab.\n",
    "\n",
    "This assignment is meant to introduce you to how we will be using Python in this course. For this assignment, you should write/type your answers into this worksheet. You may discuss the problem set with your class mates, but every student must do their own work. \n",
    "\n",
    "It is always important to cite our references that help us in our work. Please list the students you work with here:\n",
    "\n",
    "1\\. \n",
    "\n",
    "2\\.\n",
    "\n",
    "3\\.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1be8a",
   "metadata": {},
   "source": [
    "## I. PRELIMINARIES \n",
    "\n",
    "Preliminaries are listed in the HW1 Assignment on Canvas. It includes the following:\n",
    "\n",
    "* Downloading and installing Python/Anaconda\n",
    "\n",
    "* Installing necessary pacakges for the homework assignment\n",
    "\n",
    "* Setting up your GitHub account for keeping track of your work\n",
    "\n",
    "* How to sumbit your homework assignment and code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3796b2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nasdaq-data-link\n",
      "  Downloading Nasdaq_Data_Link-1.0.4-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from nasdaq-data-link) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from nasdaq-data-link) (2.28.1)\n",
      "Requirement already satisfied: six in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from nasdaq-data-link) (1.16.0)\n",
      "Collecting inflection>=0.3.1\n",
      "  Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Collecting pandas>=0.14\n",
      "  Downloading pandas-2.2.0-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "     --------------------------------------- 11.6/11.6 MB 29.7 MB/s eta 0:00:00\n",
      "Collecting more-itertools\n",
      "  Downloading more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.0/57.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.8 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from nasdaq-data-link) (1.26.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from pandas>=0.14->nasdaq-data-link) (2022.7)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "     ---------------------------------------- 346.6/346.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from requests>=2.7.0->nasdaq-data-link) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from requests>=2.7.0->nasdaq-data-link) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from requests>=2.7.0->nasdaq-data-link) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from requests>=2.7.0->nasdaq-data-link) (2023.11.17)\n",
      "Installing collected packages: tzdata, more-itertools, inflection, pandas, nasdaq-data-link\n",
      "Successfully installed inflection-0.5.1 more-itertools-10.2.0 nasdaq-data-link-1.0.4 pandas-2.2.0 tzdata-2023.4\n",
      "Collecting notebook\n",
      "  Using cached notebook-7.0.7-py3-none-any.whl (4.0 MB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from notebook) (6.4)\n",
      "Collecting jupyterlab-server<3,>=2.22.1\n",
      "  Using cached jupyterlab_server-2.25.2-py3-none-any.whl (58 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0\n",
      "  Using cached jupyter_server-2.12.5-py3-none-any.whl (380 kB)\n",
      "Collecting jupyterlab<5,>=4.0.2\n",
      "  Using cached jupyterlab-4.0.12-py3-none-any.whl (9.2 MB)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from notebook) (0.2.2)\n",
      "Collecting jupyter-server-terminals\n",
      "  Using cached jupyter_server_terminals-0.5.2-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (6.5.4)\n",
      "Collecting overrides\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (3.1.2)\n",
      "Collecting jupyter-client>=7.4.4\n",
      "  Using cached jupyter_client-8.6.0-py3-none-any.whl (105 kB)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.17.1)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.14.1)\n",
      "Collecting jupyter-events>=0.9.0\n",
      "  Using cached jupyter_events-0.9.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.58.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (21.3.0)\n",
      "Requirement already satisfied: pywinpty in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (2.0.10)\n",
      "Collecting jupyter-core!=5.0.*,>=4.12\n",
      "  Using cached jupyter_core-5.7.1-py3-none-any.whl (28 kB)\n",
      "Collecting send2trash>=1.8.2\n",
      "  Using cached Send2Trash-1.8.2-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (23.0)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.7.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (3.5.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.14.1)\n",
      "Collecting pyzmq>=24\n",
      "  Using cached pyzmq-25.1.2-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "Collecting jupyter-lsp>=2.0.0\n",
      "  Using cached jupyter_lsp-2.2.2-py3-none-any.whl (68 kB)\n",
      "Collecting async-lru>=1.0.0\n",
      "  Using cached async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from jupyterlab<5,>=4.0.2->notebook) (6.9.2)\n",
      "Requirement already satisfied: tomli in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyterlab<5,>=4.0.2->notebook) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyterlab<5,>=4.0.2->notebook) (6.0.0)\n",
      "Collecting requests>=2.31\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook) (2.11.0)\n",
      "Collecting jsonschema>=4.18.0\n",
      "  Using cached jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook) (0.9.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (3.4)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from async-lru>=1.0.0->jupyterlab<5,>=4.0.2->notebook) (4.5.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.22.1->notebook) (2022.7)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.3->jupyterlab<5,>=4.0.2->notebook) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jinja2->jupyter-server<3,>=2.4.0->notebook) (2.1.1)\n",
      "Collecting referencing>=0.28.4\n",
      "  Using cached referencing-0.33.0-py3-none-any.whl (26 kB)\n",
      "Collecting attrs>=22.2.0\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Using cached rpds_py-0.17.1-cp39-none-win_amd64.whl (206 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook) (2.8.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook) (303)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook) (2.5.2)\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (4.12.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (4.9.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (4.1.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.2.1)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.7.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.5.13)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.1.2)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (2.11.2)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook) (2.16.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook) (2023.11.17)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook) (21.2.0)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook) (1.5.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook) (5.9.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook) (8.1.1)\n",
      "Collecting jupyter-client>=7.4.4\n",
      "  Using cached jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook) (1.5.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook) (0.1.3)\n",
      "Requirement already satisfied: six in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from websocket-client->jupyter-server<3,>=2.4.0->notebook) (1.16.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (66.0.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (3.0.28)\n",
      "Requirement already satisfied: colorama in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (0.4.6)\n",
      "Requirement already satisfied: backcall in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (0.2.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook) (1.13)\n",
      "Requirement already satisfied: uri-template in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook) (1.3.0)\n",
      "Collecting fqdn\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting isoduration\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook) (2.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (2.4)\n",
      "Requirement already satisfied: webencodings in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.5.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook) (2.21)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (0.2.5)\n",
      "Collecting arrow>=0.15.0\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: executing in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (0.2.2)\n",
      "Requirement already satisfied: asttokens in c:\\users\\nadya\\appdata\\roaming\\python\\python39\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyterlab<5,>=4.0.2->notebook) (2.0.5)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\nadya\\anaconda3\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook) (2.8.19.20240106)\n",
      "Installing collected packages: send2trash, rpds-py, rfc3986-validator, rfc3339-validator, requests, pyzmq, python-json-logger, overrides, jupyter-core, fqdn, attrs, async-lru, referencing, jupyter-server-terminals, jupyter-client, arrow, jsonschema-specifications, isoduration, jsonschema, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, notebook\n",
      "  Attempting uninstall: send2trash\n",
      "    Found existing installation: Send2Trash 1.8.0\n",
      "    Uninstalling Send2Trash-1.8.0:\n",
      "      Successfully uninstalled Send2Trash-1.8.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 22.3.0\n",
      "    Uninstalling pyzmq-22.3.0:\n",
      "      Successfully uninstalled pyzmq-22.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Nadya\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\\\~yzmq.libs\\\\libsodium-138090d4.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! pip install nasdaq-data-link\n",
    "! pip install notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a1accb-310f-47d3-9fac-6c5d7029395a",
   "metadata": {},
   "source": [
    "## II.\tNUMERICAL DATA \n",
    "\n",
    "### 1. Zillow Data\n",
    "\n",
    "Access Zillow Real Estate Data using the Nasdaq Data Link API. Nasdaq Data Link is a dataset aggregation website that also has other economics datasets. These types of websites can make it easier to get data and to explore what types of datasets are available. \n",
    "    \n",
    "- Set up free account with Nasdaq Data Link (https://data.nasdaq.com/). Find your API Key in your Account Settings. You will need this to download the data.\n",
    "\n",
    "- Find the “Zillow Real Estate Data” that is Free (https://data.nasdaq.com/databases/ZILLOW/data) This will be the data you will download.\n",
    "\n",
    "- Click on the “Usage” tab, then select the “Python” sub-tab for instructions on using the Nasdaq Data Link API.\n",
    "\n",
    "- To decide which variables and regions we want to download data for, we will first download information on the indicators and regions. In a python environment, you will run the code above to import packages, setup your API connection, and download the indicator and region dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "033393dc-a223-4d56-a609-d18f0c64a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nasdaqdatalink\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "# Change the API key to yours\n",
    "#nasdaqdatalink.ApiConfig.api_key = 'YOUR_API_KEY'\n",
    "#df_zillow_indicators = nasdaqdatalink.get_table('ZILLOW/INDICATORS', paginate=True)\n",
    "#df_zillow_regions = nasdaqdatalink.get_table('ZILLOW/REGIONS', paginate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a6581f-0f58-4fb4-8153-a0ca7605c633",
   "metadata": {},
   "source": [
    "#### 1a. What does ZHVI in the indicator descriptions stand for? \n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66542de",
   "metadata": {},
   "source": [
    "#### 1b. What is the indicator, description, and category of row 38 in df_zillow_indicators? \n",
    "\n",
    "Hint: use `.iloc[]` \n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694011c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412310d5",
   "metadata": {},
   "source": [
    "#### 1c. In df_zillow_regions, how many regions are there when you search for “Providence;RI”? What is the region_id number for Providence, RI? \n",
    "\n",
    "Hint: use `.str.contains('Providence;RI')` \n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5069c",
   "metadata": {},
   "source": [
    "#### 1d. Download a dataframe the city of Providence, RI on ZHVI Single-Family Home values with the correct indicator and region IDs entered using the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be6077-40cf-4e86-9a87-c1ed1cd67a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zillow_sfh = nasdaqdatalink.get_table('ZILLOW/DATA', indicator_id=' ' , region_id=' ',paginate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebaa801-231f-48cb-b815-b82e3838c64d",
   "metadata": {},
   "source": [
    "### 2. Descriptive statistics \n",
    "\n",
    "#### 2a. What is the data frequency in df_zillow_sfh? \n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc8f52a",
   "metadata": {},
   "source": [
    "#### 2b. What is the median dollar value of a home in df_zillow_sfh? \n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11da2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a4691",
   "metadata": {},
   "source": [
    "#### 2c. What is the median dollar value of a home in df_zillow_sfh for the year of 2020? \n",
    "\n",
    "Hint: use  `[df_zillow_sfh['date'].dt.year==2020]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba99fd1-8807-4e72-8a2f-cf6f058e4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c09f0-9ee6-430e-a370-ac6fa2f63b14",
   "metadata": {},
   "source": [
    "### 3. Visualize the Data\n",
    "\n",
    "#### 3a. Plot a time series graph for values df_zillow_sfh. Be sure to title your graph and label your axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6885079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a90ad69",
   "metadata": {},
   "source": [
    "#### 3b. Plot time series graph for *yearly median* values df_zillow_sfh. Be sure to title your graph and label your axes. \n",
    "\n",
    "Hint: you will can create a new dataframe by creating a ‘year’ column using .dt.year and then use `.groupby(by=['year']).median(numeric_only=True)` to make a yearly dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a367b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the questions above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad8e24",
   "metadata": {},
   "source": [
    "#### 3c. What looks different in these graphs? Why?\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ee237",
   "metadata": {},
   "source": [
    "#### 3d. Describe the patterns in the graph. What does it say about the housing market in Providence, RI over time? In recent years? What additional data would you need to make claims about what is changing this price? \n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf832c-39ba-4143-8790-2e89c72ee11d",
   "metadata": {},
   "source": [
    "## III\tTEXT DATA \n",
    "\n",
    "### 4. US News Data\n",
    "Download US Economic News Dataset from Kaggle.com: Sign up for a free account with Kaggle.com. This website hosts data science competitions and often has cool datasets available for download. We will be using the US Economic News Dataset at https://www.kaggle.com/heeraldedhia/us-economic-news-articles. Download the CSV file from the website by clicking “Download.”\n",
    "     \n",
    "Load a subset of the data into Jupyter/Spyder/Python: Sometimes you may be working with a large dataset and it is therefore important to understand how to load a subset of the data at a time. The US Economic News dataset has 8,000 observations.\n",
    "\n",
    "#### 4a. Run the code below and explain in words each of the lines of code with comments (use # to comment): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6b60c-27bb-4247-95cd-9127a8779da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "#\n",
    "folder_path = 'TYPE FILE PATH TO WHERE YOU HAVE THE DATA'\n",
    "\n",
    "#\n",
    "fileReader = open(os.path.join(folder_path, \"US-Economic-News.csv\"), \"r\", encoding=\"unicode_escape\")\n",
    "csvReader = csv.reader(fileReader)\n",
    "\n",
    "#\n",
    "fileWriter = open(os.path.join(folder_path, \"Subset_US_Economic_News.csv\"), \"w\", encoding=\"unicode_escape\", newline='')\n",
    "csvWriter = csv.writer(fileWriter)\n",
    "\n",
    "#\n",
    "acHeader = next(csvReader)\n",
    "csvWriter.writerow(acHeader)\n",
    "\n",
    "#\n",
    "for index, acRow in enumerate(csvReader):\n",
    "    if index < 800:\n",
    "        csvWriter.writerow(acRow)\n",
    "\n",
    "#\n",
    "fileReader.close()\n",
    "fileWriter.close()\n",
    "\n",
    "#\n",
    "df_news = pd.read_csv(os.path.join(folder_path,\"Subset_US_Economic_News.csv\"), encoding='unicode_escape')\n",
    "df_news['date'] = pd.to_datetime(df_news['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25414d6",
   "metadata": {},
   "source": [
    "#### 4b.  What code would you write to keep only the ‘date’, ‘headline’, and ‘text’ columns in the dataframe? Write and run that code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa68b4-5c8d-45a8-ad9a-79be7f725d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) for b.:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e9110-f565-418d-b0d1-14713f54987b",
   "metadata": {},
   "source": [
    "### 5. String Extraction\n",
    "\n",
    "This dataframe is full of text data about US Economic News. When we try to extract information from text, formatting of words and string in code is very important.\n",
    "\n",
    "#### 5a. Count the number of headlines that have ‘US’ in them. \n",
    "\n",
    "Hint: loop over `df_news[‘headlines’]`. \n",
    "\n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd12f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the question above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797927bb",
   "metadata": {},
   "source": [
    "#### 5b. Count the number of headlines that have ‘us’ in them.\n",
    "\n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc6d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the question above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5a8f0",
   "metadata": {},
   "source": [
    "#### 5c. Why are these counts different? \n",
    "\n",
    "Hint: tell python to check if ‘us’ is in the string ‘trust’. Then tell python to check if ‘ us ‘ is in the string ‘trust’. \n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57089a48-fb8d-4b1a-8d1b-e8fe02d0bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code you need (if any) to answer the question above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412a30c-3bd1-473d-8c73-ca99abe8f634",
   "metadata": {},
   "source": [
    "### 6. Annotate Text Processing Code\n",
    "\n",
    "In text analysis, we will need to perform a few tasks to clean the data to prepare it for consistent analysis. \n",
    "\n",
    "#### 6a. Run the code and explain what each line does as comments (use # to comment): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88807593-27ec-433e-a98a-af5b3db35126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "#\n",
    "table_punctuation = str.maketrans('', '', '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~') \n",
    "\n",
    "#\n",
    "token_list = []\n",
    "for i, row in enumerate(df_news['text']):\n",
    "    text = row.translate(table_punctuation)\n",
    "    tokens = [word.lower() for word in nltk.tokenize.word_tokenize(text) if word.lower() not in stops]\n",
    "    token_list.append(tokens)\n",
    "\n",
    "#\n",
    "df_news['tokens'] = token_list\n",
    "\n",
    "#\n",
    "monetary_policy_wordlist = ['monetary', 'fed ', 'federal reserve', 'Federal Reserve', 'Monetary']\n",
    "\n",
    "#\n",
    "tally = 0\n",
    "monetary_text = []\n",
    "for row in df_news['text']:\n",
    "    mon = 0\n",
    "    if any(keyword in row for keyword in monetary_policy_wordlist):\n",
    "        tally += 1\n",
    "        mon = 1\n",
    "    monetary_text.append(mon)\n",
    "print(tally)\n",
    "df_news['monetary_flag'] = monetary_text\n",
    "\n",
    "#\n",
    "df_monetarynews = df_news[df_news['monetary_flag']==1]\n",
    "df_nonmonetarynews = df_news[df_news['monetary_flag']!=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fda93-8a92-4dc7-8d4d-a54d8ab7bd38",
   "metadata": {},
   "source": [
    "### 7. Compare Monetary News\n",
    "\n",
    "Compare and contrast the news articles about monetary policy in the US and those about non-monetary-policy economics in the US.\n",
    "\n",
    "#### 7a. Adapt the below code to answer the subsequent questions (i-iii)\n",
    "```python\n",
    "# This code calculates the top 30 most common words in df_news. \n",
    "from collections import Counter\n",
    "top_N = 30\n",
    "words = [word for tokenlist in df_news['tokens'].tolist() for word in tokenlist]\n",
    "topwords = pd.DataFrame(Counter(words).most_common(top_N),\n",
    "                           columns=['Word', 'Count']).set_index('Word')\n",
    "print(topwords)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23cfa6f",
   "metadata": {},
   "source": [
    "#### 7a.i. What are the 15 most common words from df_monetarynews?\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f74d0d-830a-4d28-8707-86d613a39e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to answer the question above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dfff15",
   "metadata": {},
   "source": [
    "#### 7a.ii. What are the 15 most common words from df_nonmonetarynews?\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f303a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to answer the question above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4430b7d7",
   "metadata": {},
   "source": [
    "#### 7a.iii. What differences do you notice?\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ce7de-c88e-4c0b-903a-187d2ff6aad7",
   "metadata": {},
   "source": [
    "#### 7b. Visualize the word use in the different types of articles using a word cloud. Below is the code for making the word cloud for the df_news dataframe. You must adapt it to the other dataframes: \n",
    "```python\n",
    "from wordcloud import WordCloud\n",
    "allwords = ' '.join(words)\n",
    "word_cloud = WordCloud(collocations=False, background_color='white').generate(allwords)\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title('Word Cloud for US Economics Articles')\n",
    "plt.show()\n",
    "``` \n",
    "    \n",
    "i. What is the word cloud for df_monetarynews? (3 points)\n",
    "\n",
    "ii. What is the word cloud df_nonmonetarynews? (3 points)      \n",
    "\n",
    "iii. What differences do you notice? Do these differences seem consistent with your list of top 15 most common words? (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2c9ff",
   "metadata": {},
   "source": [
    "#### 7b.i. What is the word cloud for df_monetarynews?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to answer the question above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb97182",
   "metadata": {},
   "source": [
    "#### 7b.ii. What is the word cloud for df_nonmonetarynews?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to answer the question above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ea7bb",
   "metadata": {},
   "source": [
    "#### 7b.iii. What differences do you notice? Do these differences seem consistent with your list of top 15 most common words?\n",
    "\n",
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5b7a9-bc8f-4c37-82cb-5523bb0c9ad6",
   "metadata": {},
   "source": [
    "### 8. Monetary Uncertainty in the News: \n",
    "\n",
    "Loughran and McDonald (2011) have created a commonly used bank of word-sentiment lists. One list is a list of “uncertainty words.” You can find this dataset in the Github HW1 Repository. \n",
    "\n",
    "The following is code to make a Monetary Uncertain Score from df_monetarynews and to plot the figure over time. However, there are three things wrong with in the code.\n",
    "\n",
    "#### 8a. Identify the typos, run the correct code, and insert the graph below. \n",
    "\n",
    "HINT: Run the code line by line and manually view the objects that were created and/or the error codes that appear.\n",
    "\n",
    "Answer: The three typos are...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6900bbc-6f3a-4b10-baf2-c68578211563",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'TYPE FILE PATH TO WHERE YOU HAVE THE DATA'\n",
    "\n",
    "# Word Lists\n",
    "uncertainty_wordlist_LM = pd.read_csv(os.path.join(folder_path,\"LM_Uncertainty.csv\"), encoding='utf-8')\n",
    "uncertainty_wordlist_LM = uncertainty_wordlist_LM['uncertain words'].tolist()\n",
    "\n",
    "# Text Uncertainty Score for Each Article\n",
    "uncertainty_score = []\n",
    "for row in df_monetarynews['tokens']:\n",
    "    u_tally = 0\n",
    "    for word in uncertainty_wordlist_LM:\n",
    "        if word in row:\n",
    "            u_tally += 1\n",
    "uncertainty_score.append(u_tally)\n",
    "                \n",
    "df_monetarynews['text_uncertainty_score'] = uncertainty_score     \n",
    "    \n",
    "# Plot Yearly Mean Monetary Policy Uncertainty Over Time\n",
    "\n",
    "#Take mean over years\n",
    "df_monetarynews['year'] = df_monetarynews['date'].dt.year.astype(str)\n",
    "df_monetarynews_yearly = df_monetarynews.groupby(by=['year']).average()\n",
    "\n",
    "df_monetarynews_yearly['year'] = pd.to_datetime(df_monetarynews_yearly.index)\n",
    "\n",
    "#Plot time Series\n",
    "plt.plot(df_monetarynews_yearly['year'],df_monetarynews['text_uncertainty_score'])\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Mean Uncertainty Score, Yearly')\n",
    "plt.title('Uncertainty in US Monetary Policy News Articles')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
